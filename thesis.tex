\documentclass[msc,deptreport,cs]{infthesis} % Do not change except to add your degree (see above).

%% Imports poached from frankly
%% STILL can't find what makes \figrule work
\usepackage{natbib}
\usepackage{mathpartir}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{thmtools,thm-restate}
\usepackage{comment}
\usepackage{flushend}
\usepackage{listings}

%% \usepackage{beramono}

%% \lstdefinestyle{mystyle}{
%%     %% backgroundcolor=\color{backcolour},
%%     %% commentstyle=\color{codegreen},
%%     %% keywordstyle=\color{magenta},
%%     %% numberstyle=\tiny\color{codegray},
%%     %% stringstyle=\color{codepurple},
%%     breakatwhitespace=false,
%%     breaklines=true,
%%     captionpos=b,
%%     keepspaces=true,
%%     %% numbers=left,
%%     %% numbersep=5pt,
%%     showspaces=false,
%%     showstringspaces=false,
%%     showtabs=false,
%%     tabsize=2,
%%     %% basicstyle=\small\ttfamily
%%     basicstyle=\ttfamily\footnotesize,
%%     %% breaklines=true
%% }

%% \lstset{style=mystyle}

%% \lstset{escapeinside={<@}{@>}}

\lstset{
  %% basicstyle=\small\ttfamily\bfseries,
  basicstyle=\footnotesize\ttfamily\bfseries,
  %% basicstyle=\footnotesize\ttfamily,
  %% basicstyle=\small\ttfamily,
  breaklines=true,
  % For having lighter-coloured text inside listings
  % https://tex.stackexchange.com/questions/144448/color-a-text-line-in-a-code-lstlisting
  escapeinside={<@}{@>}
}


\usepackage{stmaryrd}
\usepackage{natbib}
\usepackage{xspace}
%% \usepackage[pdftex,
%%             pdfauthor={Sam Lindley, Conor McBride, and Craig McLauglin},
%%             pdftitle={Doo bee doo bee doo}]{hyperref}
%\hypersetup{colorlinks=true,citecolor=blue,linkcolor=blue}
%% \hypersetup{colorlinks=true,allcolors=black}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{url}

% get rid of hypertext link on \citeauthor
\usepackage{etoolbox}

\usepackage{amssymb}

\usepackage{mathtools} % allows flush-left align environments and paired
                       % delimiters.
                       %
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}


%% abstract for inline code
\newcommand{\code}[1]{\lstinline{#1}}


\newcommand{\highlight}[1]{%
  \colorbox{red!20}{$\displaystyle#1$}}

\newcommand{\texthighlight}[1]{%
  \colorbox{red!20}{#1}}

\newcommand{\highlightout}[1]{%
  \colorbox{black!20}{$\displaystyle#1$}}

\newcommand{\greytext}[1]{\textcolor{black!40}{#1}}

\newcommand\aeff{{\AE}ff\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Start of inference rules typesetting business
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\counter}{{\color{blue}c_y}}
\newcommand{\justc}[1]{{\color{blue} c({#1})}}
\newcommand{\yieldc}{{\color{blue}{\textsf{yield}}}}
\newcommand{\plusc}{{\color{blue} +_c}}
\newcommand{\threshc}{t_y}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\newcommand{\todo}[1]
           {{\par\noindent\small\color{RoyalPurple}
  \framebox{\parbox{\dimexpr\linewidth-2\fboxsep-2\fboxrule}
    {\textbf{TODO:} #1}}}}

\newcommand{\interrupt}[1]{!(#1)}

\newcommand{\fighead}{\textbf}

\newcommand{\lameff}{$\lambda_{\text{eff}}$\xspace}
\newcommand{\lameffrow}{$\lambda_{\text{eff}}^\rho$\xspace}
\newcommand{\feff}{$F_\textrm{eff}$\xspace}
\newcommand{\impeff}{Implicit \lameff}
\newcommand\Frank{\emph{Frank}\xspace}

\newcommand\Cse{\textbf{Case}}

\newcommand{\set}[1]{\{#1\}}
\newcommand{\many}{\overline}
\newcommand{\opt}[1]{#1^?}
\newcommand{\medvert}{\mid}

\newcommand{\sem}[1]{\llbracket{#1}\rrbracket}
\newcommand{\seml}{\left\llbracket}
\newcommand{\semr}{\right\rrbracket}

\newcommand{\mdo}{~\textbf{do}~}
\newcommand{\seq}{~\textbf{;}~}
\newcommand{\assn}[2]{{#1}~\leftarrow~{#2}}
\newcommand{\func}[2]{\text{#1}~{#2}}

\newcommand{\deno}[1]{\sem{#1}\rho}
\newcommand{\denoex}[2]{\sem{#1}#2}
\newcommand{\pc}[1]{\llparenthesis{#1}\rrparenthesis}

\newcommand{\TyVar}{\mathit{Var}}
\newcommand{\dom}{\mathit{dom}}
%\newcommand{\sub}{\subseteq}
\newcommand{\Star}{{\Large$\star$}}

\newcommand{\reducesto}{\longrightarrow}

\newcommand\ba{\begin{array}}
\newcommand\ea{\end{array}}

\newcommand{\bl}{\ba[t]{@{}l@{}}}
\newcommand{\el}{\ea}

\newcommand{\bstack}{\begin{array}[t]{@{}l@{}}}
\newcommand{\estack}{\end{array}}

\newenvironment{equations}{\[\ba{@{}r@{~}c@{~}l@{}}}{\ea\]\ignorespacesafterend}
\newenvironment{eqs}{\ba{@{}r@{~}c@{~}l@{}}}{\ea}

\newenvironment{clauses}{\ba{@{}l@{~}c@{~}l@{}}}{\ea}

\newenvironment{syn}{\ba{@{}l@{~}r@{~}c@{~}l@{}}}{\ea}

\newenvironment{syntax}{\[\ba{@{}l@{~}r@{~}c@{~}l@{}}}{\ea\]\ignorespacesafterend}

\newcommand{\judgeword}[1]{~\mathbf{#1}~}

%\renewcommand{\sig}{\Sigma}
%\renewcommand{\sigs}{\Sigma s}
\newcommand{\sigentails}[1]{\mathbin{[{\text{\scriptsize ${#1}$}}]\hspace{-0.4ex}\text{-\!-}}\,}

%% \newcommand{\sigmodels}[1]{\mathbin{[{\text{\scriptsize ${#1}$}}]\!\mathord{=}}\,}
% \newcommand{\sigentails}[1]{\vdash_{#1}}

\newcommand{\val}[3]  {#1 \vdash {#2} : {#3}}

\newcommand{\rt}[1]{\langle{#1}\rangle}   % returner type

\newcommand{\valg}{\val{\Gamma}}

%% \newcommand{\is}[4]  {#1 \sigentails{#2} {#3} \judgeword{is} {#4}}
%% \newcommand{\isgs}{\is{\Gamma}{\sigs}}

%% \newcommand{\cdoes}[4]{#1 \sigentails{#2} {#3} \judgeword{has} {#4}
%% \newcommand{\cdoesgs}{\cdoes{\Gamma}{\sigs}}


%% some options for rendering bidirectional typing judgements

%% \newcommand{\inferbase}[4]{#2 \mathbin{#1} {#3} \in {#4}}
%% \newcommand{\checkbase}[4]{#2 \mathbin{#1} {#3} \ni {#4}}
%% \newcommand{\patbase}[4]{{#3} \mathbin{:} {#2} \mathbin{#1} {#4}}

\newcommand{\kindcheckbase}[3]{#2 \mathbin{#1} #3} % For well-kindedness of types
\newcommand{\inferbase}[5]{#1; #3 \mathbin{#2} {#4} \Rightarrow {#5}}
\newcommand{\checkbase}[5]{#1; #3 \mathbin{#2} #5 \mathbin{:} #4}
\newcommand{\patbase}[5]{{#1} \vdash {#4} \mathbin{:} {#3} \mathbin{#2} {#5}}
\newcommand{\bindbase}[4]{{#3} \mathbin{:} {#2} \mathbin{#1} {#4}}

%% \newcommand{\inferbase}[4]{#2 \mathbin{#1} {#3} \Rightarrow {#4}}
%% \newcommand{\checkbase}[4]{#2 \mathbin{#1} {#4} \Leftarrow {#3}}
%% \newcommand{\patbase}[4]{{#3} \mathbin{:} {#2} \mathbin{#1} {#4}}

%% \newcommand{\inferbase}[4]{#2 \mathbin{#1} {#3} \uparrow {#4}}
%% \newcommand{\checkbase}[4]{#2 \mathbin{#1} {#4} \downarrow {#3}}
%% \newcommand{\patbase}[4]{{#3} \mathbin{:} {#2} \mathbin{#1} {#4}}

%% \newcommand{\inferbase}[4]{#2 \mathbin{#1} {#3} \judgeword{infers} {#4}}
%% \newcommand{\checkbase}[4]{#2 \mathbin{#1} {#3} \judgeword{checks} {#4}}
%% \newcommand{\patbase}[4]{{#2} \judgeword{matches} {#3} \mathbin{#1} #4}

\newcommand{\makes}[5]{\inferbase{#1}{\sigentails{#3}}{#2}{#4}{#5}}
\newcommand{\has}[5]{\checkbase{#1}{\sigentails{#3}}{#2}{#4}{#5}}
\newcommand{\does}[4]{\checkbase{#1}{\vdash}{#2}{#3}{#4}}
\newcommand{\can}[4]{\makes{\kenv}{#1}{#2}{#3}{#4}}

\newcommand{\effs}[2]{{#1} \judgeword{does} {#2}}


% redefinitions for cbv type system
\newcommand{\kindchecks}[2]{\kindcheckbase{\vdash}{#1}{#2}} % Checks that a type is well-kinded
\newcommand{\infers}{\makes}
\newcommand{\checks}{\has}
\newcommand{\checksdef}{\does}
\newcommand{\matchesc}{\matches}
\newcommand{\matchesck}{\matchesc{\kenv}}

\newcommand{\infersk}{\makes{\kenv}}
\newcommand{\checksk}{\has{\kenv}}
\newcommand{\checksdefk}{\does{\kenv}}

\newcommand{\kindchecksk}{\kindchecks{\kenv}} % Checks that a type is well-kinded
\newcommand{\inferskgs}{\makes{\kenv}{\Gamma}{\sigs}}
\newcommand{\checkskgs}{\has{\kenv}{\Gamma}{\sigs}}
\newcommand{\checksdefkg}{\does{\kenv}{\Gamma}}


\newcommand{\adj}{\Delta}
\newcommand{\adapt}{\Theta}
\newcommand{\ext}{\Xi}
\newcommand{\sigs}{\Sigma}
\newcommand{\sig}{I}

\newcommand{\seed}{\sigma}

\newcommand{\effbox}[1]{[#1]}

\newcommand{\key}[1]{\mathbf{#1}} % keyword
\newcommand{\var}{\mathit}        % local variable or meta variable
\newcommand{\defaultvarname}[0]{x}

\newcommand{\op}{\mathsf}  % operator (command or computation)
\newcommand{\con}{\mathsf} % constructor (type or data)
\newcommand{\inter}{\mathsf} % interface
\newcommand{\str}[1]{\textrm{``#1''}} % string literal


\newcommand{\handleSymbol}{\rightarrow}
\newcommand{\handle}[2]{{#1} \handleSymbol {#2}}

\newcommand{\thunk}[1]{\{{#1}\}}

\newcommand{\force}[1]{{#1}!}

\newcommand{\emptylist}{[]}
\newcommand{\cons}{\mathbin{::}}
\newcommand{\concat}{\,\texttt{++}\,} %mathbin{+\!+}}
%\newcommand{\snoc}{\mathbin{:<}}
\newcommand{\snoc}{\ }


\newcommand{\NN}{\mathbb{N}}

\newcommand\slab[1]{(\textrm{#1})}

\newcommand{\ev}{E}
\newcommand{\evd}{\varepsilon}

\newcommand{\effin}[1]{\langle {#1} \rangle}
\newcommand{\effout}[1]{[{#1}]}

\newcommand{\nowt}{\emptyset}
\newcommand{\id}{\iota}
\newcommand{\pid}{\var{s}} % Pattern identity variable

\newcommand{\EC}{\mathcal{E}}
\newcommand{\EF}{\mathcal{F}}
\newcommand{\PC}{\mathcal{P}} % Syntactic phrase class for af operation
\newcommand{\venv}{\theta}

\newcommand{\freeze}{\ceil}

\newcommand{\uc}{\mathord{\downarrow}}
\newcommand{\cu}{\mathord{\uparrow}}

\newcommand{\redtou}{\leadsto_{\mathrm{u}}}
\newcommand{\redtoc}{\leadsto_{\mathrm{c}}}
\newcommand{\stepsto}{\longrightarrow}

\newcommand{\stepstou}{\longrightarrow_{\mathrm{u}}}
\newcommand{\stepstoc}{\longrightarrow_{\mathrm{c}}}

\newcommand{\sigat}{\mathbin{@}}

\newcommand{\meta}{\mathsf}
\newcommand{\level}{\meta{level}}
\newcommand{\af}{\meta{af}}
\newcommand{\handles}{~\meta{handles}~}

\newcommand{\poised}{~\meta{poisedfor}~}
\newcommand{\insts}{\meta{inst}}
\newcommand{\remap}{\meta{remap}}

\newcommand{\sigyields}[1]
           {\mathbin{\text{-\!-\!}[{\text{\scriptsize ${#1}$}}]\,}}

\newcommand{\matches}[5]{\patbase{#1}{\sigyields{#4}}{#2}{#3}{#5}}
\newcommand{\matchesv}[4]{\patbase{#1}{\dashv}{#2}{#3}{#4}}
\newcommand{\matchesvk}{\matchesv{\kenv}}

\newcommand{\bindsv}[4]{\bindbase{\dashv}{#2 \leftarrow #3}{#1}{#4}}
\newcommand{\bindsc}[5]{\bindbase{\sigyields{#4}}{#2 \leftarrow #3}{#1}{#5}}

\newcommand{\letin}[4][\defaultvarname]
           {\key{let}\;{#1}:{#2}={#3}\;\key{in}\;{#4}}
\newcommand{\letxin}[3][\defaultvarname]
           {\key{let}\;{#1}={#2}\;\key{in}\;{#3}}
\newcommand{\letrec}[4][f]{\key{letrec}~\many{{#1}:{#2} = {#3}}~\key{in}~{#4}}
\newcommand{\letrecU}[3][f]{\key{letrec}~\many{{#1} = {#2}}~\key{in}~{#3}}
\newcommand{\Gt}{\theta} % Substitution meta variable
\newcommand{\submap}[2]{{{#1}\vDash{#2}}}
\newcommand{\sub}[4]{#1 \vdash {{#2}:\submap{{#3}}{{#4}}}}
\newcommand{\subk}{\sub{\kenv}}
\newcommand{\subext}[2]{{{#1}{#2}}}
\newcommand{\subst}[3][\defaultvarname]{{#2}[{#3}/{#1}]}

% Frank letrec substitution
\newcommand{\recsub}[5][f]
      {[\many{\cu (\thunk{\many{\many{#2}\mapsto\letrec[{#1}]{#5}{#3}{#4}}}
            : {#5})/{#1}}]}


%%%% START inference rule system for action of adjustment on ability %%%%
\newcommand{\semi}{;}
\newcommand{\kenv}{\Phi}  % kind environment
\newcommand{\kenva}{\Psi} % another kind environment
%% \newcommand{\kenv}{\mathcal{T}} % kind environment
\newcommand{\ienv}{\Omega} % Instance environment
\newcommand{\adjact}[3]{{#1}\vdash{#2}\dashv{#3}}
\newcommand{\adpcom}[5]{{{#1}\vdash{#2}({#3} \to {#4})\dashv{#5}}}
\newcommand{\itrbnd}[5]{{{#1}\vdash{#2}:{#3}\dashv{#4}\semi{#5}}}
% \newcommand{\wf}[2]{{{#1}\vdash{#2}}}
\newcommand{\itrinst}[4]{{{#1}\vdash{#2}:{#3}\dashv{#4}}}

%%% END inference rule system for action of adjustment on ability %%%%%

% Untyped machine letrec substitution
\newcommand{\recsubst}[5]
 {{#1}[\many{(\thunk{\many{\many{#2} \mapsto \letrecU{#3}{#4}}}:{#5})/f}]}

%% Abstract machine commands
% Typing
\newcommand{\HAbs}[2]{{{#1}\to{#2}}}

\newcommand{\fail}{\textbf{fail}}

%% Translation function: Frank Terms to Untyped A-Normal Form
\newcommand{\UANF}[1]{{\llbracket{{#1}}\rrbracket}}

% Terms
\newcommand{\mtrns}[3][]{{#2} & \Rightarrow^{#1} & {#3}} % For array env
\newcommand{\mtrnsR}[3][]{{#2}\Rightarrow^{#1}{#3}}

\newcommand{\confg}[2]{{\langle{{#1}},{{#2}}\rangle}}
\newcommand{\term}[3]
           {{\langle{{#1}},{{#2}}\rangle\downarrow{#3}}}

\newcommand{\admin}[2]{{\langle{{#1}}\mid{{#2}}\rangle}}
\newcommand{\mat}[3]
           {{\langle{{#1}}\mid{{#2}}\mid{{#3}}\rangle}}
\newcommand{\matc}[5]
        {{\langle{{#1}}\mid{{#2}}\mid{{#3}}\mid{{#4}}\mid{{#5}}\rangle}}

\newcommand{\msub}[3][\defaultvarname]{{#2}[{#1}\mapsto{#3}]}

\newcommand{\FHan}[4][\many{\effin{\adj}}]{{({#2}:{#1},{#3}\mid{#4})}}
\newcommand{\FSeq}[2][\defaultvarname]
           {{({#1}.{#2})}}
\newcommand{\SCons}[2]{{{#1}\circ{#2}}}


\newcommand{\HSHan}[5][C]{{{#2}\circ({#3},{#1},{#4}\mid{#5})}}
\newcommand{\HSSeq}[4][\defaultvarname]
           {{#2}\circ({#1}:{#3}.{#4})}
\newcommand{\HSCons}[2]{{{#1}\circ{#2}}}
\newcommand{\NF}[2]{{{#1}~\star~{#2}}}

\newcommand{\evalto}{\Longrightarrow}


\newcommand{\para}[1]{\paragraph{#1.}}

\newcommand{\gor}{\mid}
\newcommand{\pipe}{\texttt{|}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% End of inference rules typesetting business
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Start of main text
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\begin{preliminary}

\title{Asynchronous Effect Handling 2}

\author{Leo Poulson}

\abstract{
  Features for asynchronous programming are commonplace in the programming
  languages of today, allowing programmers to issue tasks to run on other
  threads and wait for the results to come back later. This is particularly
  useful for programs like web programs, etc...

  In this thesis we show how asynchronous programming can be very easily
  accomodated in a language with existing support for effect handlers. We show
  how, with a small change to the language implementation, truly asynchronous
  programming with pre-emptive concurrency is achieved.
}

\maketitle

\section*{Acknowledgements}
thanks!

\tableofcontents

\end{preliminary}

\chapter{Introduction}

Effects, such as state and nondeterminism, are pervasive when programming; for a
program to do anything beyond compute a pure mathematical function, it must
interact with the outside world, be this to read from a file, make some random
choice, or run concurrently with another program. Algebraic effects and their
handlers are a novel way to encapsulate, reason about and specify computational
effects in programming languages. For instance, a program that reads from and
writes to some local state can utilise the \textsf{State} effect, which supports
two \emph{operations}; \textsf{get} and \textsf{put}. A handler for the
\textsf{State} effect gives a meaning to these abstract operations.

Programming with effects is increasingly popular, references, ...

Traditional effect handling is \emph{synchronous}; when an effectful operation
is invoked, the rest of the computation pauses whilst the effect handler
performs the requisite computation and then resumes the original caller.
\todo{Do we need to speak about how the continuation of the caller gets 'offered
  up'?}
%
For many effects, this blocking behaviour is not a problem; the handler usually
returns quickly, and the user does not notice anything. Of course, not every
possible computational effect behaves like this. Consider an effect involving a
query to a remote database. We might not want to block the rest of the
computation whilst we perform this, as the query might take a long time; this
case is even stronger if we do not immediately want the data. To support this
kind of behaviour, we need to be able to invoke and handle effects in an
asynchronous, non-blocking manner.

\texthighlight{In this project we investigate the implementation and
  applications of asynchronous effects}. Our lens for this is the language
Frank~(\cite{convent2020doo}), a functional programming designed with effect
handlers at its core. We follow the design
of~\aeff~(\cite{ahman2020asynchronous}), a small programming language designed
around asynchronous effects but supporting little else. We show how, with a
small change to the semantics of Frank, we can recreate the asynchronous effect
handling behaviour of~\aeff.

\todo{Talk about extra examples...}

%% The changes made to Frank are straightforward to recreate, and whilst the Frank
%% library for programming with asynchronous effects enjoys some specific

Frank is a well-suited language for an asynchronous effects library, especially
because of the fine-grained control over suspended computations, making it very
easy to treat code as data. Despite this, our approach does not use any specific
Frank features; furthermore, the changes made to the semantics of Frank are
easily recreateable. It is our hope that these methods could be recreated in
another language equipped with first-class effect handlers. Effect handlers have
already proven to make complicated control flow easy to implement
(\textbf{refs}), and our work further cements this viewpoint.

\section{Related Work}

TFP, aeff, etc
\section{Contributions}

\paragraph*{Asynchronous Effects Library} We present a library for programming
with asynchronous effects in the style of \aeff, built in Frank. We show how a
complex system can be expressed concisely and elegantly when programming in a
language with effect handlers, further cementing the case for effects as a
foundation for concurrent programming. \todo{Rewrite the end of this; slightly
  messy}.

\paragraph*{Pre-emptive Concurrency} We show how, by making a small change to
the operational semantics of Frank, we achieve pre-emptive concurrency; that is,
the suspension of running threads \emph{without} co-operation. It is our hope
that this change is simple enough to be transferrable to other languages.

\paragraph*{Examples} We also deliver a set of examples of the uses of
asynchronous effects, and show how they have benefits to other models.

\section{Structure}

In Chapter~\ref{chap:programming-in-frank} we give an introduction to
programming with effects in Frank. We skip over some unneeded parts, such as
adaptors, in the interests of time.

In Chapter~\ref{chap:formalisation} we give the formalisation of Frank. Again,
we skip over extraneous details which can be seen in past work
(\cite{convent2020doo}), opting to only describe the parts needed to understand
the changes to the semantics for the following chapter.

In Chapter~\ref{chap:preemptive-concurrency} we show how by making a small
change to the semantics of Frank we yield pre-emptible threads; that is, we can
interrupt a function in the same 'co-operative' style but without co-operation

\todo{Fix above}

In Chapter~\ref{chap:implementation} we describe the implementation of our
asynchronous effect handling library in Frank. In Chapter~\ref{chap:examples} we
give examples of the new programs that become easily expressible when combined
with the changes made in Chapter~\ref{chap:preemptive-concurrency}.

In Chapter~\ref{chap:conclusion} we conclude.






\chapter{Programming in Frank}
\label{chap:programming-in-frank}

Frank is a functional programming language, designed with the use of algebraic
effects at its heart. As such, Frank has an effect type system used to track
which effects a computation may use.

Frank also offers very fine-grained control over computations. It clearly
distinguishes between computation and value, and offers \emph{multihandlers} to
carefully control when computations are evaluated. This combined with effect
handling provides a very rich foundation for expressing complex control
structures.

In this chapter, we introduce the language, and show why it is so well-suited to
our task. We assume some familiarity with typed functional programming, and skip
over some more traditional aspects of the language --- algebraic data types,
pattern matching, etc --- so we can spend more time with the novel, interesting
parts of the language.

\section{Types, Values and Operators}

Frank types are distinguished between \emph{effect types} and \emph{value
  types}. Value types are the standard notion of type; effect types are
used to describe where certain effects can be performed and handled.

Value types are further divided into traditional data types, such a \code{Bool},
\code{List X}, and \emph{computation types}. Computation types are used to
define an operator; they come in the general form \code{\{<Effect> X -> ... ->
  [Effects] Y\}}. This type expresses that the operator can handle some effect
in the first argument and then performs some other effects as a result,
returning a value of type \code{Y}.

Frank then specialises effect handling to traditional function application; a
function is the special case of an operator that handles no arguments. We see
that a function type \code{\{X -> Y -> Z\}} is just a special case of the
general operator type where no effects are handled or performed. Throughout this
thesis, we call

Thunks then are the special case of an $n$-ary function that takes 0 arguments.
A thunk, being a suspended computation, that will result in a value of type
\code{Bool} has type \code{\{Bool\}}. This gives us fine-grained control over
executing computations, and also translates to control over when to execute
effects; this proves to be very useful later.

\todo{Example --- maybe fire missiles one?}

\section{Effects and Effect Handling}

\paragraph*{Interfaces and Operations}

Frank encapsulates effects through \emph{interfaces}, which offer
\emph{commands}. For instance, the \code{State} effect (interface) offers two
operations (commands), \code{get} and \code{put}. In Frank, this translates to

\begin{lstlisting}
  interface State X = get : X
                    | put : X -> Unit

  interface RandInt = random : Int
\end{lstlisting}

The type signatures of the operations mean that \code{get} is a 0-ary operation
which is \emph{resumed} with a value of type \code{X}, and \code{put} takes a value of
type \code{X} and is resumed with \code{unit}. Programs get access to an interface's
command by including them in the \emph{ability} of the program. Commands are
invoked just as normal functions;

\begin{lstlisting}
  xplusplus : {[State Int] Unit}
  xplusplus! = put (get! + 1)
\end{lstlisting}

\noindent This familiar program increments the integer in the state by 1.

\paragraph*{Handling Operations}

Traditioanl functions in Frank are a specialisation of Frank's handlers; that is
to say, functions are handlers that handle no effects. A handler for an
interface pattern matches \emph{on the operations} that are invoked, as well as
on the \emph{values} that the computation can return. Furthermore, the handler
gets access to the \emph{continuation} of the calling function as a first-class
value. Consider the handler for \code{State};

\begin{lstlisting}
  runState : {<State S> X -> S -> X}
  runState <get -> k>   s = runState (k s) s
  runState <put s -> k> _ = runState (k unit) s
  runState x            _ = x
\end{lstlisting}

\noindent The type of \code{runState} expresses that the first argument is a
computation that can perform \code{State S} effects and will eventually return a
value of type \code{X}, whilst the second argument is a value of type \code{S}.
%could remove this below line...
The \code{State S} effect is then \emph{removed} from the first argument.

What happens when we run \code{runState xplusplus! 0}? When a computation is
invoked, it is performed until it results in either a \emph{value} or a
\emph{command}. Thus, \code{runState} will be paused until \code{xplusplus!}
reduces; \code{runState} is resumed when \code{xplusplus} is in one of these two
forms.

\code{xplusplus} instantly invokes \code{get!}. At this point, control is given
to the handler \code{runState}; both in the sense that \code{runState} is now
being executed by the interpreter, and that \code{runState} has control over the
\emph{continuation} of \code{xplusplus}, which is a function of type \code{Int
  -> [State Int] Unit}. We see that \code{runState} chooses to resume this
continuation with the value of the state at that time.

\paragraph*{Effect Forwarding}

Effects that are not handled by a particular handler are left to be forwarded up
to the next highest one. For instance, we might want to write a random number to
the state;

\begin{lstlisting}
  xplusrand : {[State Int, RandomInt] Unit}
  xplusrand! = put (random!)
\end{lstlisting}

\noindent We then have to handle both the \code{State Int} and \code{Random}
effect in this computation. Of course, we could just define one handler for both
effects; however in the interests of \emph{modularity} we want to define two
different handlers for each effect and \emph{compose} them. We can reuse the
same \code{runState} handler from before, and define a new handler for
\code{RandomInt};

\begin{lstlisting}
  runRand : {Int -> <RandomInt> X -> X}
  runRand seed <random -> k> = runRand (mod (seed + 7) 10) (k seed)
  runRand _ x = x
\end{lstlisting}

\noindent And compose them in the comfortable manner, by writing \code{runRand
  (runState xplusrand!)}.

\todo{Maybe show example of how the order of composition can change the ending
  semantics --- a la state + aborting}

\paragraph*{Top-Level Effects}
Some effects need to be handled outside of pure Frank, as Frank is not
expressive or capable enough on its own. Examples are console I/O, web requests,
and ML-style state cells. These effects will pass through the whole stack of
handlers up to the top-level, at which point they are handled by the
interpreter.






\paragraph*{Implicit Effect Polymorphism}

Consider the type of the well-known function \code{map} in Frank;

\begin{lstlisting}
  map : {{X -> Y} -> List X -> List Y}
  map f [] = []
  map f (x :: xs) = (f x) :: (map f xs)
\end{lstlisting}

\noindent One might expect that the program \code{map \{_ -> random!\} [1, 2, 3]}
would give a type error; we are mapping a function of type \code{\{Int ->
  [RandomInt] Int\}}, which does not match the argument type \code{\{X -> Y\}}.
However, Frank uses a shorthand for \emph{implicit effect variables}. The
desugared type of \code{map} is actually

\begin{lstlisting}[mathescape]
  map : {{X -> [$\epsilon$|] Y} -> List X -> [$\epsilon$|] List Y}
\end{lstlisting}

\noindent This type expresses that whatever the ability is of \code{map f xs} will be
offered to the element-wise operator \code{f}. As such, the following
typechecks;

\begin{lstlisting}
  writeRand : {List Int -> [RandomInt] List Int}
  writeRand xs = map {_ -> random!} xs
\end{lstlisting}

\todo{Talk about deliberately stopping this}

\todo{Talk about what the bar means. }

A similar thing happens in interface declarations. We might define the
\code{Choose} effect, which non-deterministically asks for one of two
computations to be picked for it to continue with;

\begin{lstlisting}
  interface Choose X =
      choose : {[Choose X] X} -> {[Choose X] X} -> X
\end{lstlisting}

\noindent This definition desugars to

\begin{lstlisting}[mathescape]
  interface Choose X [$\epsilon$] =
      choose : {[$\epsilon$| Choose X] X} -> {[$\epsilon$| Choose X] X} -> X
\end{lstlisting}

\noindent Once again, an implicit effect variable is inserted in every ability
available.




\paragraph*{Synchronicity and Conversations}
Observe how the interaction between the effect invoking function and the handler
of this effect becomes like a conversation; the caller asks the handler for a
response to an operation, and the caller will then wait, blocking, for a
response. This can be characterised as \emph{synchronous} effect handling.

But what if we want to make a request for information, then do something else,
then pick up the result later when we need it? This is the canonical example of
asynchronous programming. It is not as simple as just invoking our e|.g.
\code{getRequest} effect; computation would block once this is invoked, meaning
we are stuck waiting for the request to return.

This asynchrony is exactly what we search for in this project.

%% Frank's realisation of effects is through \emph{interfaces} which offer
%% \emph{commands}. For instance, the \code{State} effect (interface) offers two
%% operations (commands), \code{get} and \code{put}. In Frank, this translates to

%% \begin{lstlisting}
%%   interface State X = get : X
%%                     | put : X -> Unit
%% \end{lstlisting}

%% The type signatures of the operations mean that \code{get!}, when handled, will
%% be a value of type \code{X}, whilst \code{put x} once handled will be a value of
%% type \code{Unit}.

%% \code{get} and \code{put} are given meaning by defining a \emph{handler} for the
%% \code{State} interface. Handlers in Frank are very simple things; they take as
%% argument a computation that can perform effects. The handlers then pattern match
%% \emph{on the operations} that are invoked, as well as on the \emph{values} that the
%% computation can return. What makes effects not just a special kind of value is
%% that they also offer the \emph{continuation} of the caller as a first-class
%% value.

%% Here we display the handler for \code{State};

%% \begin{lstlisting}
%%   runState : {<State X> Y -> X -> Y}
%%   runState <get -> k>   s = runState (k s) s
%%   runState <put s -> k> _ = runState (k unit) s
%%   runState y            _ = y
%% \end{lstlisting}

\paragraph*{Multihandlers}
\label{para:multihandlers}

Recall that in Frank pure functions are just the special case of handlers that
handle no effects. Naturally, this notion extends to the $n$-ary case; we can
handle multiple effects from different sources are once. Handlers which handle
multiple effects simultaneously are unsurprisingly called \emph{multihandlers}.
This lets us write functions such as \code{pipe} (example due to \cite{convent2020doo});

\begin{lstlisting}[numbers=left]
interface Send X = send : X -> Unit

interface Receive X = receive : X
 
pipe : {<Send X>Unit -> <Receive X>Y -> [Abort]Y}
pipe <send x -> s> <receive -> r> = pipe (s unit) (r x)
pipe <_> y = y
pipe unit <_> = abort!
\end{lstlisting}

Line 5 states that \code{pipe} will handle all instances of the \code{Send}
effect in the first argument, all instances of the \code{Receive} effect in the
second, and might perform \code{Abort} commands along the way. The matching
clauses are also new to the reader; line 6 implements the communication between
the two functions. We reinvoke \code{pipe}, passing the payload \code{x} of
\code{send} to the continuation of \code{r}. Lines 7 and 8 make use of the
\emph{catch-all} pattern, \code{<m>}. This will match the invocation of any
effect that is handled by that argument, or a value, binding this to \code{m}.
In line 7, the catchall pattern matches either a \code{send} command or a value;
in this case, the receiver has produced a value, so we can return that. In line
8 \code{<_>} matches either a value or a \code{receive}; but it must be a
\code{receive} command, as the value case would have been caught above. Hence we
have a broken pipe, so the \code{abort} command is invoked. This can then be
caught by another handler, which can implement a recovery strategy.

\todo{Is it worth changing the example to match request on the left earlier?}

\paragraph*{Polymorphic Commands}

As well as having polymorphic interfaces, such as \code{State X}, parametrised
by e|.g.~the data stored in the state, Frank supports polymorphic
\emph{commands}. These are commands which can be instantiated for any type. An
example is ML-style references, realised through the \code{RefState} interface;

\begin{lstlisting}
  interface RefState = new X   : X -> Ref X
                     | read X  : Ref X -> X
                     | write X : Ref X -> X -> Unit
\end{lstlisting}

\noindent For instance, \code{new X} can be instantiated by supplying a value as
an argument. A \code{Ref X} cell is then returned as answer.

\section{Cooperative Concurrency}
\label{sec:concurrency}

Frank is a single-threaded language. It is fortunate, then, that effect handlers
give us a malleable way to run multiple program-threads ``simultaneously'' \todo
{This is poorly written --- fix}.

This is because the invocation of an operation not only offers up the
operation's payload, but also the \emph{continuation} of the calling
computation. The handler for this operation is then free to do what it pleases
with the continuation. For many effects, such as \code{getState}, nothing
interesting happens to the continuation; in the case of \code{getState}, it is
resumed with the value in state. But these continuations are first-class; they
can resumed, sure, but also stored elsewhere or even thrown away. As such, by
handling \code{Yield} operations, we easily pause and switch between several
threads.

\subsection{Simple Scheduling}
\label{subsec:simple-scheduling}

We introduce some simple program threads and some scheduling multihandlers, to
demonstrate how subtly different handlers generate different scheduling
strategies.

\begin{lstlisting}
interface Yield = yield : Unit

words : {[Console, Yield] Unit}
words! = print "one "; yield!; print "two "; yield!; print "three "; yield!

numbers : {[Console, Yield] Unit}
numbers! = print "1 "; yield!; print "2 "; yield!; print "3 "; yield!
\end{lstlisting}

First note the simplicity of the \code{Yield} interface; we have one operation
supported, which looks very boring; the operation \code{yield!} will just return
unit --- of course, it is the way we \emph{handle} yield that is more
interesting.

\begin{lstlisting}
-- Runs all of the LHS first, then the RHS.
scheduleA : {<Yield> Unit -> <Yield> Unit -> Unit}
scheduleA <yield -> m> <n> = scheduleA (m unit) n!
scheduleA <m> <yield -> n> = scheduleA m! (n unit)
scheduleA _ _ = unit

-- Lets two yields synchronise, then handles both
scheduleB : {<Yield> Unit -> <Yield> Unit -> Unit}
scheduleB <yield -> m> <yield -> n> = scheduleB (m unit) (n unit)
scheduleB <yield -> m> <n> = scheduleB (m unit) n!
scheduleB <m> <yield -> n> = scheduleB m! (n unit)
scheduleB _ _ = unit
\end{lstlisting}

\todo{Can maybe delete the 2nd and 3rd matches of scheduleB to make the point
  more clear?}

We see two multihandlers above. Each take two \code{yield}ing threads and
schedule them, letting one run at a time. \code{scheduleA} runs the first thread
to completion, and only then runs the second one; the first time that the second
thread \code{yield}s it is \emph{blocked}, and can no longer execute. As such,
the output of \code{scheduleA words! numbers!} is \code{one 1 two three 2 3
  unit}.

\code{scheduleB} is fairer and more profound. We run \code{scheduleB words!
  number!} and receive \code{one 1 two 2 three 3 unit}; \code{scheduleB} is fair
and will ``match'' the yields together. We step through slowly. First
\code{words!} will print \code{one}, then it will \code{yield}. At this point
--- recalling that multihandlers pattern match left-to-right --- the second
thread, \code{numbers!}, is allowed to execute. In the meantime, \code{words!}
is stuck as \code{<yield -> m>}; it cannot evaluate any further, it is
\emph{blocked}. Whilst \code{words} is blocked \code{numbers!} prints \code{1}
and then \code{yield}s. Great; now the first case matches. Both threads are
resumed and the process repeats itself.

\todo{ The second paragraph here is a more compelling explanation; maybe we can
  just get rid of all of the scheduleA business and /just/ have the scheduleB
  stuff? scheduleA is quite obvious i think whilst B is more subtle and compelling. }

\todo{ It's not true that it matches L-R as much as runs all computations L - R
  until they are all a command / value - fix this }

\subsection{Forking New Processes}

We can make use of Frank's higher-order effects to dynamically create new
threads at runtime. We strengthen the \code{Yield} interface by adding a new
operation \code{fork};

\begin{lstlisting}
  interface Co = fork : {[Co] Unit} -> Unit
               | yield : Unit
               | exit : Unit
\end{lstlisting}

The type of \code{fork} expresses that \code{fork} takes a suspended computation
that can perform further \code{Co} effects, and returns unit when handled. We
can now run programs that allocate new threads at runtime, such as the below

\begin{lstlisting}
forker : {[Console, Co [Console]] Unit}
forker! = print "Starting! ";
          fork {print "one "; yield!; print "two "};
          fork {print "1 "; yield!; print "2 "};
          exit!
\end{lstlisting}

We can now choose a strategy for handling \code{fork} operations; we can either
lazily run them, by finishing executing the current thread, or eagerly run them,
suspending the currently executing thread and running the forked process. The
handler for the former, breadth-first style of scheduling, is

\begin{lstlisting}
scheduleBF : {<Co> Unit -> [Queue Proc] Unit}
scheduleBF <fork p -> k> =
    enqueue (proc {scheduleBF (<Queue> p!)});
    scheduleBF (k unit)
scheduleBF <yield -> k> =
    enqueue (proc {scheduleBF (k unit)});
    runNext!
scheduleBF <exit -> _> =
    runNext!
scheduleBF unit =
    runNext!
\end{lstlisting}

Notice the use of the \code{Queue} effect; we have to handle the computation
\code{scheduleBF forker!} with a handler for \code{Queue} effects afterwards.
Moreover, notice how concisely we can express the scheduler; this is due to the
handler having access to te continuation of the caller, and treating it as a
first-class object that can be stored elsewhere. We can see a diagram of how
\code{scheduleBF} treats continuations in Figure \ref{fig:scheduleBF}, and a
similar diagram of how the depth-first handling differs in Figure \ref{fig:scheduleDF}.

\begin{figure}[t]
  \centering
  \begin{subfigure}
    \includegraphics[width=0.8\textwidth]{imgs/scheduleBF.png}
    \caption{Breadth-First scheduling}\label{fig:scheduleBF}
  \end{subfigure}
  \begin{subfigure}
    \includegraphics[width=0.8\textwidth]{imgs/scheduleDF.png}
    \caption{Depth-First scheduling}\label{fig:scheduleDF}
  \end{subfigure}
\end{figure}

\chapter{Formalisation of Frank}
\label{chap:formalisation}

%% We now discuss the formalisation of the Frank language. This has been discussed
%% in depth in previous work (\cite{convent2020doo}), so we do not go into great
%% detail about some parts of the language. Indeed, we skip over much of it for
%% brevity's sake. We do, however ht

The formalisation of the Frank language has been discussed at length in previous
work~(\cite{convent2020doo}). However, in order to illustrate changes made to
the language to get pre-emptive concurrency, we explain some of the key parts of
the language in this section.

\begin{figure}[h]  %\figrule
\[
\ba{@{}c@{}}
\ba{@{}c@{\quad\quad}c@{}}
\begin{syn}
  \slab{data types}            & D \\
  \slab{value type variables}  & X \\
  \slab{effect type variables} & E| \\
  \slab{value types}           & A, B   &::= & D~\overline{R} \\
                               &        &\gor& \thunk{C} \gor X \\
  \slab{computation types}     & C      &::= & \many{T \to}~G \\
  \slab{argument types}        & T      &::= & \effin{\adj}A \\
  \slab{return types}          & G      &::= & \effout{\sigs}A \\

  \slab{type binders}          & Z      &::= & X \gor [E]\\
  \slab{type arguments}        & R      &::= & A \gor [\Sigma]\\
  \slab{polytypes}             & P      &::= & \forall \overline{Z}.A \\
\end{syn}
&
\begin{syn}
  \slab{interfaces}           & I \\
  \slab{term variables}       & x, y, z, f \\
  \slab{instance variables}   & \pid, a, b, c \\
  \slab{seeds}                & \seed  &::= & \nowt \gor \ev \\
  \slab{abilities}            & \sigs  &::= & \seed\pipe\ext \\
  \slab{extensions}           & \ext   &::= & \id \gor \ext, \sig~\many{R} \\
  \slab{adaptors}             & \adapt &::= & \id \gor \adapt, \sig(S \to S') \\
  \slab{adjustments}          & \adj   &::= & \adapt\pipe\ext \\
  \slab{instance patterns}    & S      &::= & \pid \gor S \snoc a \\
  \slab{kind environments}    & \kenv,
                                \kenva &::= & \cdot \gor \kenv, Z \\
  \slab{type environments}    & \Gamma &::= & \cdot \gor \Gamma, x:A %\\
%                              &        &    & \hphantom{\cdot}
                                              \gor \Gamma, f:P\\
 \slab{instance environments} & \ienv  &::= & \pid:\sigs \gor \ienv, a:\sig~\many{R}\\
\end{syn} \\
\ea \\
\ea
\]
\\[0.25cm]

\caption{Types}
\label{fig:types}
%\figrule
\end{figure}

Value types are either datatypes instantiated with type arguments $D~\overline{R}$, thunked
computations $\thunk{C}$, or value type variables $X$. Computation types are
zero or more argument types $T$ resulting in a return type $G$. A computation of
type

\[
C = \effin{\adapt_1\pipe\ext_1}A_1 \to \dots \to
      \effin{\adapt_k\pipe\ext_k}A_k \to \effout{\sigs}B
\]

has arugment types $\effin{\adapt_i\pipe\ext_i}A_i$ and return type
$\effout{\sigs}B$; that is, a computation of type $C$ handles effects in
extensions $\ext_i$ in its $i$-hth argument. It then returns a value of type $B$
and potentially performs effects in $\sigs$.

\todo{Talk about adaptors at each index?}

An ability $\sigs$ \ldots. It may be closed $\nowt$ or open $\ev$.
%
An extension $\ext$ is a finite list of interfaces.

We deliberately leave out details on adaptors for the sake of brevity. We also
skip over the typing rules, as they are standard. These can be seen in the
appendix.

\begin{figure} %\figrule
\begin{syntax}
  %% \slab{monomorphic term variables} & x, y, z \\
  %% \slab{polymorphic term variables} & f \\
  \slab{constructors}               & k \\
  \slab{commands}                   & c \\
  \slab{uses}                 & m      &::= &
     x \gor f~\many{R} \gor m~\many{n} \gor \cu(n:A) \\
  \slab{constructions}        & n      &::= &
    \uc m \gor k~\many{n} \gor c~\many{R}~\many{n} \gor \thunk{e} \\
                              &        &\gor& \key{let}~f : P = n~\key{in}~n'
                                   \gor
                                   \key{letrec}~\many{f : P = e}~\key{in}~n \\
                              &        &\gor&  \effin{\adapt}~n \\
  \slab{computations}         & e      &::=& \many{\many{r} \mapsto n}
  \\
  \slab{computation patterns} & r      &::=& p
                                        \gor \effin{\handle{c~\many{p}\,}{z}}
                                        \gor \effin{x} \\
  \slab{value patterns}       & p      &::=& k~\many{p} \gor x        \\
\end{syntax}
\\[0.25cm]
%\textit{with} term variables $x$, $y$, $z$, polymorphic term variables $f$, constructors $k$, commands $c$\\[0.25cm]
\caption{Terms}
\label{fig:terms}
% \figrule
\end{figure}

\paragraph*{Terms} Frank uses bidirectional typing (\cite{pierce2000local}); as such, terms are
split into \emph{uses} whose types are inferred, and \emph{constructions}, which
are checked against a type. Uses are monomorphic variables $x$, polymorphic
variable instantiations $f~\many{R}$, applications $m~\many{n}$ and type
ascriptions $\cu(n:A)$. Constructions are made up of uses $\uc m$, data
constructor instances $k~\many{n}$, suspended computations $\thunk{e}$, let
bindings $\key{let}~f : P = n~\key{in}~n'$, recursive let $\key{letrec}~\many{f
  : P = e}~\key{in}~n$ and adaptors $\effin{\adapt}~n$. We can inject a use into
a construction with $\uc$ and vice versa ($\cu$); in real Frank code these are
not present.

Computations are produced by defining a sequence of pattern matching clauses.
Each pattern matching clause takes a sequence $\many{r}$ of computation
patterns. These can either be a request pattern
$\effin{\handle{c~\many{p}\,}{z}}$, a catch-all pattern $\effin{x}$, or a
standard value pattern $p$. Value patterns are made up of data constructor
patterns $k~\many{p}$ or variable patterns $x$.

\todo{Talk about typing for Frozen commands --- basically jus say that they
  retain the type when frozen.}

\paragraph*{Runtime Syntax}

The operational semantics uses the runtime syntax of
Figure~\ref{fig:runtime-syntax}. The uses and constructions are supplemented
with a special term $\freeze{\EC[c~\many{R}~\many{w}]}$, of \emph{frozen
  commands}. We discuss these further later.

We distinguish use and construction values, and then further separate
construction values into uses and non-uses. We also declare a new class of
\emph{normal forms}, to be used in pattern binding.

Finally we have evaluation contexts, which are sequences of evaluation frames.
The interesting case is $u~(\many{t}, [~],\many{n})$; it is this that gives us
left-to-right evaluation of multihandler arguments.

\begin{figure}[t]
%% \figrule
\begin{syntax}
\slab{uses}                    & m   &::= & \dots \mid \freeze{\EC[c~\many{R}~\many{w}]} \\
\slab{constructions}           & n   &::= & \dots \mid \freeze{\EC[c~\many{R}~\many{w}]} \\
\slab{use values}              & u   &::= & x \gor f~\many{R} \gor \cu (v : A) \\
\slab{non-use values}          & v   &::= & k~\many{w} \gor \thunk{e} \\
\slab{construction values}     & w   &::= & \uc u \gor v \\
\slab{normal forms}            & t   &::= & w \gor \freeze{\EC[c~\many{R}~\many{w}]} \\
\slab{evaluation frames}       & \EF &::= & [~]~\many{n}
                                      \gor  u~(\many{t},[~],\many{n})
                                      \gor  \cu([~]:A) \\
                               &     &\gor& \uc [~]
                                      \gor  k~(\many{w},[~],\many{n})
                                      \gor  c~\many{R}~(\many{w},[~],\many{n}) \\
                               &     &\gor& \key{let}~f: P = [~]~\key{in}~n
                                      \gor \effin{\adapt}~[~] \\
\slab{evaluation contexts}     & \EC &::= & [~] \gor \EF[\EC] \\
\end{syntax}
\caption{Runtime Syntax}
\label{fig:runtime-syntax}
%% \figrule
\end{figure}

\paragraph*{Operational Semantics} Finally, the operational semantics are given
in Figure~\ref{fig:operational-semantics}.

The essential rule here is \textsc{R-Handle}. This relies on a new relations
regarding \emph{pattern binding} (Figure~\ref{fig:pattern-binding}). We discuss
these rules in more detail later. $\bindsc{r}{T}{t}{\sigs}{\venv}$ states that
the computation pattern $r$ of type $T$ and ability $\sigs$ matches the normal
form $t$ yielding substitution $\venv$. The index $k$ is then the index of the
earliest ``line'' of pattern matches that all match. The conclusion of the rule
states that we then perform the substitutions $\many{\venv}$ that we get on the
return value $n_k$ to get our result. This is given type $B$.

\todo{Tighten up description.}

\textsc{R-Ascribe-Use} and \textsc{R-Ascribe-Cons} remove unneeded conversions
from use to construction. \textsc{R-Let} and \textsc{R-LetRec} are standard.
\textsc{R-Adapt} shows that an adaptor applied to a value is the identity.

We have several rules regarding the freezing of commands. When handling a
command, we need to capture its delimited continuation; that is, the largest
enclosing evaluation context that does \emph{not} handle it.
\textsc{R-Freeze-Comm} shows how commands, once used, become frozen;
\textsc{R-Freeze-Frame-Use} and \textsc{R-Freeze-Frame-Cons} show how the rest
of the context becomes frozen. These two rules rely on the predicate $\EC
\textsf{ handles } c$. This is true if the context does indeed handle the
command $c$; i.e. it is a context of the form $u~(\many{t}, [~], \many{u'})$
where $u$ is a handler that handles $c$ at the index corresponding to the hole.
Thus, the whole term is frozen up to the first handler, at which point is it
handled with \textsc{R-Handle}}.

The $\textsc{R-Lift}$ rules then express that we can perform any of these
reductions in any evaluation context.


\todo{Frozen commands - delimited continuations}

\begin{figure}[t]
%% \figrule
\flushleft
$\boxed{m \redtou m'} \quad \boxed{n \redtoc n'} \quad \boxed{m
    \stepstou m'} \quad \boxed{n \stepstoc n'}$
\begin{mathpar}
\inferrule[R-Handle]
  {% The type system should enforce this!
   %(\handles{\adj_j}{t_j})_j \\
   k = \min_i\,\{i \mid \exists \many{\venv}.(\bindsc{r_{i,j}}{\effin{\adj_j} A_j}{t_j}{\sigs}{\venv_j})_{j}\} \\
   (\bindsc{r_{k,j}}{\effin{\adj_j} A_j}{t_j}{\sigs}{\venv_j})_{j}}
   %% \forall i < k.\exists j.r_{i, j} \# t_j}
   %% l \text{~is minimal}}
  {\cu (\thunk{((r_{i,j})_j \to n_i)_i} : \thunk{\many{\effin{\adj} A \to}~\effout{\sigs}B})~\many{t} \redtou \cu ((\many{\venv}(n_k) : B)}

\inferrule[R-Ascribe-Use]
  { }
  {\cu(\uc u:A) \redtou u}

\inferrule[R-Ascribe-Cons]
  { }
  {\uc \cu (w : A) \redtoc w}

\inferrule[R-Let]
  { }
  {\key{let}~f:P = w~\key{in}~n \redtoc n[\cu (w : P)/f]}

\inferrule[R-LetRec]
  {\many{e = \many{\many{r} \to n}}}
  {%\vphantom{\many{\many{\many{\many{f}}}}}
   \key{letrec}~\many{f:P = e}~\key{in}~n' \redtoc
    n'[\many{\cu (\thunk{\many{\many{r} \to \key{letrec}~\many{f:P = e}~\key{in}~n}}: P)/f}]}

\inferrule[R-Adapt]
  { }
  {\effin{\adapt}~w \redtoc w}

\inferrule[R-Freeze-Comm]
  { }
  {c~\many{R}~\many{w} \redtoc \freeze{c~\many{R}~\many{w}}}\\

\inferrule[R-Freeze-Frame-Use]
  {\neg(\EF[\EC] \handles c)}
  {\EF[\freeze{\EC[c~\many{R}~\many{w}]}] \redtou \freeze{\EF[\EC[c~\many{R}~\many{w}]]}}

\inferrule[R-Freeze-Frame-Cons]
  {\neg(\EF[\EC] \handles c)}
  {\EF[\freeze{\EC[c~\many{R}~\many{w}]}] \redtoc \freeze{\EF[\EC[c~\many{R}~\many{w}]]}}

\\
\inferrule[R-Lift-UU]
  {m \redtou m'}
  {\EC[m] \stepstou \EC[m']}

\inferrule[R-Lift-UC]
  {m \redtou m'}
  {\EC[m] \stepstoc \EC[m']}

\inferrule[R-Lift-CU]
  {n \redtoc n'}
  {\EC[n] \stepstou \EC[n']}

\inferrule[R-Lift-CC]
  {n \redtoc n'}
  {\EC[n] \stepstoc \EC[n']}
\end{mathpar}

\caption{Operational Semantics}
\label{fig:operational-semantics}
%% \figrule
\end{figure}

\paragraph*{Pattern Binding}

We now discuss the pattern binding rules of Figure~\ref{fig:pattern-binding}.

\begin{figure}[t]
%% \figrule
\flushleft
%\textit{Comp. pattern $r$ for $\langle \Delta \rangle A$ matches $u$ under
%amb. $\venv$ and binds $\venv$.}
$\boxed{\bindsc{r}{T}{t}{\sigs}{\venv}}$
\begin{mathpar}

\inferrule[B-Value]
  {\adjact{\sigs}{\adj}{\sigs'} \\\\ \bindsv{p}{A}{w}{\venv}}
  {\bindsc{p}{\effin{\adj}A}{w}{\sigs}{\venv}}

  \inferrule[B-Request]
    {%I~\many{R} \in \ext \\ %\capturesI{\Delta}{I}{\iota}\\
    \adjact{\sigs}{\adj}{\sigs'} \\
    \EC \poised c \\\\
    \adj = \adapt\pipe\ext \\
    c : \forall \many{Z}. \many{B \to}~B' \in \ext \\
    (\bindsv{p_i}{B_i}{w_i}{\venv_i})_i}
    {\bindsc{\effin{c~\many{p} \to z}}{\effin{\adj}A}
    {\freeze{\EC[c~\many{R}~\many{w}]}}{\sigs}{\many{\venv}[\cu (\thunk{x \mapsto \EC[x]} : \thunk{B' \to \effout{\sigs'}A})/z]}}
\\
\inferrule[B-CatchAll-Value]
  {\adjact{\sigs}{\adj}{\sigs'}}
  {\bindsc{\effin{x}}{\effin{\adj}A}{w}{\sigs}{[\cu (\thunk{w}\mathord{:}\thunk{\effout{\sigs'}A})/x]}}
\\
\inferrule[B-CatchAll-Request]
  {
  \adjact{\sigs}{\adj}{\sigs'} \\
  \EC \poised c \\\\
  \adj = \adapt\pipe\ext \\
  c : \forall \many{Z}. \many{B \to}~B' \in \ext
  }
  {\bindsc{\effin{x}}{\effin{\adj}A}
  {\freeze{\EC[c~\many{R}~\many{w}]}}
  {\sigs}
  {[\cu (\thunk{\freeze{\EC[c~\many{R}~\many{w}]}}\mathord{:}\thunk{\effout{\sigs'}A})/x]}}

\end{mathpar}


%~~ \textit{Value pattern $p$ for type $A$ matches $w$ and binds $\venv$.}
%
$\boxed{\bindsv{p}{A}{w}{\venv}}$
\begin{mathpar}

\inferrule[B-Var]
  { }
  {\bindsv{x}{A}{w}{[\cu (w : A)/x]}}

\inferrule[B-Data]
  {k~\many{A} \in D~\many{R} \\
   (\bindsv{p_i}{A_i}{w_i}{\venv_i})_i}
 {\bindsv{k~\many{p}}{D~\many{R}}{k~\many{w}}{\many{\venv}}}
\end{mathpar}

\caption{Pattern Binding}
\label{fig:pattern-binding}
%% \figrule
\end{figure}



\chapter{Pre-emptive Concurrency}
\label{chap:preemptive-concurrency}

\section{Motivation}
\label{sec:interrupt-motivation}

One important part of our asynchronous effect handling system is the ability to
interrupt arbitrary computations.
%
This is essential for pre-emptive concurrency, which relies on being able to
suspend a computation \emph{non-cooperatively}; the computation gets suspended
without being aware of its suspension.

\todo{Rewrite this - clumsy}


%% Consider the following program;

%% \begin{lstlisting}
%% interface Stop = stop : Unit

%% interface Go = go : Unit

%% suspend : {<Stop, Go>Unit -> Unit -> Maybe {Unit} -> Unit}
%% suspend <stop -> k> <cont> _ =
%%     suspend (k unit) unit (just cont)
%% suspend <go -> k>   _      (just c)   =
%%     suspend (k unit) c! nothing

%% \end{lstlisting}

%% \noindent Here the first argument to \lstinline{suspend} performs
%% \lstinline{stop} and \lstinline{go} operations, which control the execution of
%% the second thread.

Consider the two programs below;

\begin{lstlisting}
controller : {[Stop, Go, Console] Unit}
controller! =
    stop!; print "stop "; sleep 200000; go!; controller!

runner : {[Console] Unit}
runner! = print ``1 ``; print ``2 ``; print ``3 '';
\end{lstlisting}

%% \noindent We ideally want a multihandler that can run these two programs in
%% parallel, such that the console output will be \code{1 stop 2 stop 3 stop}; that
%% is to say, the \code{stop} and \code{go} operations from \code{controller} can
%% control the execution of \code{runner}.

\noindent We want a multihandler that uses the \code{stop} and \code{go} commands from
\code{controller} to control the execution of \code{runner}. The console output
of this multihandler should be then \code{1 stop 2 stop 3 stop}.

\section{Interruption with Yields}
\label{sec:yield-interruption}

%% One way we can get this behaviour is using the \code{Yield} interface. This
%% offers a single operation, \code{yield : Unit}. With this, we can write a
%% multihandler \code{suspend};

We can simulate this behaviour by using the familiar \code{Yield} interface from
Section~\ref{subsec:simple-scheduling}.

\begin{lstlisting}
runner : {[Console, Yield] Unit}
runner! = print "1 "; yield!; print "2 "; yield!; print "3 "; yield!

suspend : {<Yield> Unit -> <Stop, Go> Unit -> Maybe {[Console, Yield] Unit} -> [Console] Unit}
suspend <yield -> r> <stop -> c> _ =
    suspend unit (c unit) (just {r unit})
suspend <_>          <go -> c>   (just res) =
    suspend res! (c unit) nothing
suspend unit         <_>         _ = unit
\end{lstlisting}

\noindent Running \code{suspend runner! controller! nothing} then prints out
\code{1 stop 2 stop 3} as desired.
%
This is due to the same synchronisation behaviour that we saw in
Section~\ref{subsec:simple-scheduling}; \code{runner} is evaluated until it
becomes a command or a value, and then \code{controller} is given the same
treatment. Once both are a command or a value, pattern matching is done.

So far so good; this works as we hoped. However, observe that we had to change
the code of the \code{runner} to \code{yield} every time it prints.
%
%% We would rather not have this requirement; the threads should be suspendable
%% without knowing in advance they will be suspended, and thus without needing to
%% explicitly \code{yield}.
%
This is not in the spirit of pre-emptive concurrency; we are still operating
co-operatively. Threads should be unaware of the fact they are even being
pre-empted.
% The below line could maybe go...
Furthermore, see that the \code{yield} operation adds no more information; it is
just used as a placeholder operation; any operation would work.
%
As such, we keep searching for a better solution.

\todo{Penultimate sentence could maybe go , a bit clumsy / weird}

\section{Relaxing Catches}\label{sec:relaxingcatches}

\begin{figure}[t]
%% \figrule
%% \flushleft
%% \centering
%\textit{Comp. pattern $r$ for $\langle \Delta \rangle A$ matches $u$ under
%amb. $\venv$ and binds $\venv$.}
\begin{mathpar}

\inferrule[B-CatchAll-Request]
  {
  \adjact{\sigs}{\adj}{\sigs'} \\
  \EC \poised c \\\\
  \adj = \adapt\pipe\ext \\
  c : \forall \many{Z}. \many{B \to}~B' \in \ext
  }
  {\bindsc{\effin{x}}{\effin{\adj}A}
  {\freeze{\EC[c~\many{R}~\many{w}]}}
  {\sigs}
  {[\cu (\thunk{\freeze{\EC[c~\many{R}~\many{w}]}}\mathord{:}\thunk{\effout{\sigs'}A})/x]}}

\\
\inferrule[B-CatchAll-Request-Loose]
  {
  \adjact{\sigs}{\adj}{\sigs'} \\
  %% \EC \poised c \\\\
  %% \adj = \adapt\pipe\ext \\
  %% c : \forall \many{Z}. \many{B \to}~B' \in \ext
  }
  {\bindsc{\effin{x}}{\effin{\adj}A}
  {\freeze{\EC[c~\many{R}~\many{w}]}}
  {\sigs}
  {[\cu (\thunk{\freeze{\EC[c~\many{R}~\many{w}]}}\mathord{:}\thunk{\effout{\sigs'}A})/x]}}

\end{mathpar}

\caption{Updated \textsc{B-CatchAll-Request}}
\label{fig:loose-catchall-request}
%% \figrule
\end{figure}
The key to this lies in the catchall pattern, $\effin{x}$, and the pattern
binding rules of Figure~\ref{fig:pattern-binding}; specifically
\textsc{B-CatchAll-Request}. We quickly go into detail on this rule now.
%
$\effin{x} : {\effin{\adj}A}$ states that $\effin{x}$ is a term with value type
$A$ and \emph{adjustment} $\adj = \adapt\pipe\ext$, made up of an adaptor
$\adapt$ and an extension $\ext$. This extension is made up of a list of
interface instantiations $\sig~\many{R}$.

The crux is that the command $c$ that is invoked in the frozen term
$\freeze{\EC[c~\many{R}~\many{w}]}$ must be an element of this extension $\ext$;
that is, it must be handled by the current use of \textsc{R-Handle}. Refer back
to the example of Section~\ref{sec:yield-interruption}. This rules means that
the catch-all pattern \code{<_>} in the final pattern matching case of
\code{suspend} can match against \code{stop} or \code{go}, as they are present
in the extension of the second argument, but not \code{print} commands; although
the \code{Console} interface is present in the ability of \code{controller}, it
is not in the extension in \code{suspend}.



%% Recall that the catchall pattern $\effin{x}$ matches either a value or a command
%% that is present in the term's ability. For instance, the pattern \code{<k>} in
%% the code above matches either \code{unit} or \code{<yield -> k>}. The variable
%% \code{k} is then bound to whatever this match is, leaving the (potentially)
%% invoked command unhandled. This is expressed in the \textsc{B-CatchAll-Request}
%% rule in Figure~\ref{fig:pattern-binding} (restated in
%% Figure~\ref{fig:loose-catchall-request}).

%% \todo{'Present in the term's ability' is incorrect, fix }


%% Important to note is that only effects that are handled in that position are
%% able to be caught. \code{runner} also makes use of the \code{print} effect, but
%% these are not able to be caught by the catchall command. Formally, this is due
%% to the fourth requirement of \textsc{B-CatchAll-Request}; that the command $c$
%% that is invoked is a member of $\ext$.

\begin{figure}[t]
%% \figrule
%% \flushleft
%% \centering
%\textit{Comp. pattern $r$ for $\langle \Delta \rangle A$ matches $u$ under
%amb. $\venv$ and binds $\venv$.}
\begin{mathpar}

\inferrule[B-CatchAll-Request]
  {
  \adjact{\sigs}{\adj}{\sigs'} \\
  \EC \poised c \\\\
  \adj = \adapt\pipe\ext \\
  c : \forall \many{Z}. \many{B \to}~B' \in \ext
  }
  {\bindsc{\effin{x}}{\effin{\adj}A}
  {\freeze{\EC[c~\many{R}~\many{w}]}}
  {\sigs}
  {[\cu (\thunk{\freeze{\EC[c~\many{R}~\many{w}]}}\mathord{:}\thunk{\effout{\sigs'}A})/x]}}

\\
\inferrule[B-CatchAll-Request-Loose]
  {
  \adjact{\sigs}{\adj}{\sigs'} \\
  %% \EC \poised c \\\\
  %% \adj = \adapt\pipe\ext \\
  %% c : \forall \many{Z}. \many{B \to}~B' \in \ext
  }
  {\bindsc{\effin{x}}{\effin{\adj}A}
  {\freeze{\EC[c~\many{R}~\many{w}]}}
  {\sigs}
  {[\cu (\thunk{\freeze{\EC[c~\many{R}~\many{w}]}}\mathord{:}\thunk{\effout{\sigs'}A})/x]}}

\end{mathpar}

\caption{Updated \textsc{B-CatchAll-Request}}
\label{fig:loose-catchall-request}
%% \figrule
\end{figure}

In the interests of pre-emption, we propose to remove this constraint from
\textsc{B-CatchAll-Request}.
%
The resulting rule \textsc{B-CatchAll-Request-Loose} can be seen in
Figure~\ref{fig:loose-catchall-request}. This lets us update the previous
\code{suspend} code to the following, which yields the same results as last
time;

\begin{lstlisting}
runner : {[Console] Unit}
runner! = print "1 "; print "2 "; print "3 "

suspend : {Unit -> <Stop, Go> Unit -> Maybe {[Console] Unit} -> [Console] Unit}
suspend <r> <stop -> c> _ =
    suspend unit (c unit) (just {r unit})
suspend <_>          <go -> c>   (just res) =
    suspend res! (c unit) nothing
suspend unit         <_>         _ = unit
\end{lstlisting}

\todo{ Check that the above still works... }
\noindent Now when we run \code{suspend runner! controller! nothing}, the
\code{suspend} handler is able to

\todo{Talk about how this still maintains the ``no-snooping'' policy; we can't
  inspect or access the effects that are invoked, but we know they happen.}

Even with this extension,

\section{Interrupting Arbitrary Terms}

The approach of Section \ref{sec:relaxingcatches} can only interrupt command
invocations. If \code{runner} were instead a sequence of pure
computations\footnote{I.e. \code{runner! = 1 + 1; 1 + 1; 1 + 1; ...}}, we would
be unable to interrupt it; it does not invoke commands.

As such, we need to further change the pattern binding rules of Figure
\ref{fig:pattern-binding}. This is to let us interrupt arbitrary computation
terms. In Figure \ref{fig:runtime-syntax-freeze}, we see an updated version of
the runtime syntax; this allows for the suspension of arbitrary \emph{uses},
essentially just function applications.

\todo{verify that uses are ``just'' apps and constructions}

\begin{figure}[t]
%% \figrule
\begin{syntax}
\slab{uses}                    & m   &::= & {\dots} \gor
\freeze{\EC[c~\many{R}~\many{w}]} \gor \highlight{\freeze{m}} \\
\slab{constructions}           & n   &::= & \dots \gor
\freeze{\EC[c~\many{R}~\many{w}]} \gor \highlight{\freeze{m}} \\
\slab{use values}              & u   &::= & x \gor f~\many{R} \gor \cu (v : A) \\
\slab{non-use values}          & v   &::= & k~\many{w} \gor \thunk{e} \\
\slab{construction values}     & w   &::= & \uc u \gor v \\
\slab{normal forms}            & t   &::= & w \gor \freeze{\EC[c~\many{R}~\many{w}]} \gor \highlight{\freeze{m}}\\
\slab{evaluation frames}       & \EF &::= & [~]~\many{n}
                                      \gor  u~(\many{t},[~],\many{n})
                                      \gor  \cu([~]:A) \\
                               &     &\gor& \uc [~]
                                      \gor  k~(\many{w},[~],\many{n})
                                      \gor  c~\many{R}~(\many{w},[~],\many{n}) \\
                               &     &\gor& \key{let}~f: P = [~]~\key{in}~n
                                      \gor \effin{\adapt}~[~] \\
\slab{evaluation contexts}     & \EC &::= & [~] \gor \EF[\EC] \\
\end{syntax}
\caption{Runtime Syntax, Updated with Freezing of Uses}
\label{fig:runtime-syntax-freeze}
%% \figrule
\end{figure}


\begin{figure}[h]
%% \figrule
\flushleft
$\boxed{m \redtou m'} \quad \boxed{n \redtoc n'} \quad \boxed{m
    \stepstou m'} \quad \boxed{n \stepstoc n'}$
\begin{mathpar}

\inferrule[R-Freeze-Use]
  {  }
  { m \redtou \freeze{m} }

\inferrule[R-Freeze-Frame-Use]
  { }
  { \EF[\EC[\freeze{m}]] \redtou \freeze{\EF[\EC[m]]} }

\inferrule[R-Freeze-Frame-Cons]
  { }
  { \EF[\EC[\freeze{m}]] \redtoc \freeze{\EF[\EC[m]]} }

\end{mathpar}

\caption{Updated Freezing \todo{Maybe need to add that the eval ctx is NOT a handler?}}
\label{fig:Freezing}
%% \figrule
\end{figure}

Note that frozen terms here behave in a similar way to frozen commands, by
freezing the rest of the term around it as well. This continues up until a
handler is reached, at which point the term is unfrozen and resumed.

With this in mind, we now give the updated rule for the catchall pattern
matching on frozen terms. This can be seen in Figure \ref{fig:catchall-freeze}.
It expresses that an arbitrary frozen \emph{use} can be matched against the
computation pattern $\effin{x}$. The suspended, unfrozen computation $\thunk{m}$
is then bound to $x$, in a similar way to other \textsc{B-CatchAll} rules.

\begin{figure}[t]
%% \figrule
\flushleft
%\textit{Comp. pattern $r$ for $\langle \Delta \rangle A$ matches $u$ under
%amb. $\venv$ and binds $\venv$.}
\begin{mathpar}
\inferrule[B-CatchAll-Interrupt]
  {\adjact{\sigs}{\adj}{\sigs'}}
  {\bindsc{\effin{x}}{\effin{\adj}A}{\freeze{m}}{\sigs}{[\cu (\thunk{m}\mathord{:}\thunk{\effout{\sigs'}A})/x]}}
\end{mathpar}

\caption{Catching Interrupts rule. }
\label{fig:catchall-freeze}
%% \figrule
\end{figure}

\begin{figure}[t]
%% \figrule
\flushleft
\begin{mathpar}
%% \\
\inferrule[R-Interrupt]
  { }
  {m \redtou \freeze{m}}
%% \\
\end{mathpar}

\caption{Use interruption rule}
\label{fig:r-interrupt}
%% \figrule
\end{figure}

%% We also need to supplement the operational semantics of Figure
%% \ref{fig:small-step} with a rule for a use $m$ becoming interrupted. This extra
%% required rule can be seen in

Figure~\ref{fig:r-interrupt} shows how uses $m$ become interrupted. This rule
supplements the operational semantics of Figure~\ref{fig:small-step}. It states
that at any point, a use term can reduce to the same term but suspended. At this
point, the term cannot reduce any further; observe that $\freeze{~}$ is not an
evaluation context. The term then blocks until unfrozen. Note how similar this
is to regular command invocations, which block until their continuation is
invoked.

\todo{Check that just uses is enough}

\todo{Talk about how this achieves our goal. }

The addition of this rule introduces non-determinism into the language; at any
point, a use can either step as normal (e.g. through the \textsc{R-Handle}
rule), or it can be interrupted. An interrupted term $\interrupt{m}$ can no
longer reduce; it is blocked until it is resumed by the \textsc{R-Handle} rule.

\todo{Talk about non-determinism as a result of this}
\todo{Maybe move non-determinism to the next section?}


\section{Yielding}
\label{sec:inserting-yields}

Observe that the approach of Section \ref{sec:relaxingcatches} can only
interrupt command invocations. If \code{runner} were instead a sequence of pure
computations\footnote{I.e. \code{runner! = 1 + 1; 1 + 1; 1 + 1; ...}}, we would
be unable to interrupt it; it does not invoke commands. In this section we show
how we can easily interrupt \emph{arbitrary terms}, without defining any special
new syntax or constructions. Furthermore, we do this in a
way that lets the programmer choose how to resume the term, without being set in
stone.

Recall the simple \textsf{Yield} effect from Section~\ref{sec:concurrency}; it
supports one operation, $\textsf{yield} : \textsf{Unit}$. Its primary use is to
give the programmer access to the continuation so that they might perform some
scheduling or other control flow operations. Our solution is simple; whenever we
are in an evaluation context where the ability contains the \textsf{Yield}
effect, we insert an invocation of \textsf{yield} before the term in question.
This is expressed formally in Figure~\ref{fig:insert-yield}.

\todo{Do we need to worry about conversion from Use -> Constrction}

\begin{figure}[h]
%% \figrule
\flushleft
$\boxed{m \redtou m'} $
\begin{mathpar}
%
%
\inferrule[R-Yield-EF]
          { \EC~\textsf{allows}~\textsf{Yield} }
          { \EC[m] \redtou \EC[\textsf{yield!}; m] }
\end{mathpar}
%
%
\caption{Inserting Yields}
\label{fig:insert-yield}
%% \figrule
\end{figure}

Note that \textsc{R-Yield-EF} relies on the predicate
$\EF[\EC]~\allows~c$. This states that the \textsf{Yield}
interface is in the ability of the term in the evaluation context.

For any frame apart from argument frames,
$\EF[\EC]~\allows~c = \textsf{false}$. In this case, it is defined as
follows;

\begin{equations}
  \cu (v : \thunk{\many{\effin{\adj} A
      \to}~\effout{\sigs}B})~(\many{t},[~],\many{n})~\allows~c =
  \sigs'~\allows~c ~\text{ where } \adjact{\sigs}{\adj_{|\many{t}|}}{\sigs'}    \\
\end{equations}


\noindent For an ability $\sigs = \seed\pipe\ext$, $\sigs~\allows~c$ is true if
$c \in I$ for some $I \in \ext$.

Informally, $\EC~\allows~c$ is true when $\EC$ is a handler where the command
$c$ is a member of an interface in its ability when modified by the adaptor at
the corresponding position.

\todo{Clean this up}

We also make use of an auxiliary combinator $\_ ; \_$. This is the traditional
sequential composition operator $\textsf{snd}~x~y~\mapsto~y$, where both
arguments are evaluated and the result of the second one is returned. We see
that it would be a type error if we were to insert a \code{yield} command in a
context where \textsf{yield} was not a part of the ability.

\todo{Maybe elaborate on this --- give the type for \textsf{snd}, give the
  typing deriviation to show that insertion is well typed?}

In the context of \textsc{R-Yield-EF} this means we will perform the
\textsf{yield} effect and then the use $m$, but discard the result from
\textsf{yield}.

Observe that this gives us fine-grained control over which parts of our program
become asynchronous. One might want a short-running function to not be
pre-emptible and just run without pause; conversely, one might want a
long-running function to be interruptible. The programmer gets to choose this by
labelling the functions with \textsf{Yield} in the ability.

\section{Counting}

The semantics given by Section~\ref{sec:inserting-yields} is fine, but is
non-deterministic; at any point, we can choose to either insert a \textsf{yield}
invocation or carry on as normal. Furthermore, we do not particularly need to
yield very frequently; we might rather yield every 1000 reduction steps or so.

As such, we supplement the operational semantics with a counter $\counter$. This
counter has two states; it could either be counting up, which is the form
$\justc{n}$ for some $n$, or it is a signal to yield as soon as possible, which
is the form $\yieldc$.

To increment this counter, we use a slightly modified version of addition,
denoted $\plusc$. This is simply defined as

\begin{equations}
  x \plusc y =
          \left\{ \ba{@{}l@{\quad}l@{}}
              \justc{x + y} & \text{if } x + y \leq \threshc \\
              \yieldc & \text{otherwise}
          \right.
\end{equations}

\noindent where $\threshc$ is the threshold at which we force a yield.

The transitions in our operational semantics now become of the form $m; \counter
\redtou m'; \counter '$. In Figure~\ref{fig:counting-rules} we give an updated
rule for \textsc{R-Handle} --- overwriting the previous rule --- and two new
rules for inserting yields.

\begin{figure}
%% \figrule
\flushleft
$\boxed{m \redtou m'} \quad \boxed{n \redtoc n'} \quad \boxed{m
    \stepstou m'} \quad \boxed{n \stepstoc n'}$
\begin{mathpar}
\inferrule[R-Handle-Count]
  {% The type system should enforce this!
   %(\handles{\adj_j}{t_j})_j \\
   k = \min_i\,\{i \mid \exists \many{\venv}.(\bindsc{r_{i,j}}{\effin{\adj_j} A_j}{t_j}{\sigs}{\venv_j})_{j}\} \\
   (\bindsc{r_{k,j}}{\effin{\adj_j} A_j}{t_j}{\sigs}{\venv_j})_{j}}
   %% \forall i < k.\exists j.r_{i, j} \# t_j}
   %% l \text{~is minimal}}
  {\cu (\thunk{((r_{i,j})_j \to n_i)_i} : \thunk{\many{\effin{\adj} A
        \to}~\effout{\sigs}B})~\many{t}; \highlight{\justc{n}}~
    \redtou~
    \cu ((\many{\venv}(n_k) : B)); \highlight{n \plusc 1}}

\\
\inferrule[R-Yield-Can]
          { \EC~\textsf{allows}~\textsf{Yield} }
          { \EC[m]; \yieldc~\redtou~\EC[\textsf{yield!}; m]; \justc{0} }
\\
\inferrule[R-Yield-Can't]
          { \neg (\EC~\textsf{allows}~\textsf{Yield}) \\
            m; \justc{n} \redtou m'; c' }
          { \EC[m]; \yieldc~\redtou~\EC[m]; \yieldc }
\\

\end{mathpar}

\caption{Yielding with Counting}
\label{fig:counting-rules}
%% \figrule
\end{figure}

\textsc{R-Handle-Count} replaces the previous rule \textsc{R-Handle}. If the
counter is in the state $\justc{n}$, we perform the handling as usual,
incrementing the counter by 1. Here we use $\plusc$, which will set the counter
to be $\yieldc$ if the addition brings it over the threshold value.

\textsc{R-Yield-Can} and \textsc{R-Yield-Can't} dictate what to do if we have to
yield as soon as possible. If the evaluation context allows \textsf{yield}
commands to be inserted we do so and reset the counter. If not, but the term
could otherwise reduce if the counter had a different value, then we make that
transition, still maintaining the $\yieldc$ signal.

\citeauthor{dolan2017concurrent} take a similar approach to this when
investigating asynchrony in Multicore OCaml (\cite{dolan2017concurrent}). They
rely on the operating system to provide a timer interrupt, which is handled as a
first-class effect. Our system is more self-contained; the timing is implemented
within the language itself and doesn't rely on the operating system providing
interrupts. Furthermore, we get fine-grained control over when the timer can
fire, as we can choose to put \textsf{Yield} in the ability of interruptible
terms.

\todo{Maybe remove part about 'fine-grained'ness as they can just pick whether
  or not to handle the timer effect anyway?}


\section{Handling}
\label{sec:handling}

One of the strengths of this approach is that these inserted \textsf{yield}
commands are first-class Frank commands; the programmer can choose how to handle
them, potentially choosing some very complicated scheduling scheme.

We show how with multihandlers and the discussed modification we achieve simple
pre-emptive concurrency. Recall the scheduler from
Section~\ref{sec:concurrency}, expanded to the 3-argument case below.

\begin{lstlisting}
sch3 : {<Yield> Unit -> <Yield> Unit -> <Yield> Unit -> Unit}
sch3 <yield -> h> <yield -> j> <yield -> k> = sch3 (h unit) (j unit) (k unit)
sch3 <yield -> h> <yield -> j> <k> = sch3 (h unit) (j unit) (k!)
sch3 <yield -> h> <j> <yield -> k> = sch3 (h unit) (j unit) (k!)
\end{lstlisting}

We first try and handle both yield commands at once, then one at a time. This
pattern can generalise to the $n$-ary case in a simple manner; we first try and

\todo{Maybe the 3-ary case is too long --- could just do the 2ary one.}

\todo{Instead of this, we can just use the example from before.}

\section{Soundness}

We now state the soundness property for our extended system, as well as the
subject reduction theorem needed for this proof. Our system is nothing more than
the system of~\citeauthor{convent2020doo} with extra rules; as such we omit most of
the details.


\begin{theorem}[Subject Reduction]\label{thm:sub-red}
\vskip
\begin{itemize}\\
\item If $\inferskgs{m}{A}$ and $m; \counter \redtou m'; \counter'$ then $\inferskgs{m'}{A}$.
\item If $\checkskgs{A}{n}$ and $n; \counter \redtoc n'; \counter'$ then $\checkskgs{A}{n'}$.
\end{itemize}
  \end{theorem}

\begin{proof}
By induction on the transitions $\redtou, \redtoc$.

We first consider the two possible states for $\counter$. If it is in the form
$\justc{n}$, then the reduction rules are simply the same as
in~\cite{convent2020doo}, as we do not change the counter. The only exception to
this is the updated \textsc{R-Handle} rule, which is essentially the same except
for modifications to the counter; regardless of the counter, the resulting term
$m'$ still remains the same type.

Thus the only new cases are \textsc{R-Yield-Can} and \textsc{R-Yield-Can't}.

\begin{itemize}
\item[\Cse] \textsc{R-Yield-Can}
  By the assumption we have that $\EC~\allows~\textsf{yield}$. This only holds
  if the context is of the form
  \[\EC[~] = \cu (v : \thunk{\many{\effin{\adj} A
      \to}~\effout{\sigs}B})~(\many{t},[~],\many{n'})\]

  Assume that
  \[\inferskgs{\cu (v : \thunk{\many{\effin{\adj} A
      \to}~\effout{\sigs}B})~(\many{t},\EC'[n],\many{n'})}{B}\]

  Then by inversion on \textsc{T-App} we have
  $\checksk{\Gamma}{\sigs'_{|\many{t}|}}{A_{|\many{t}|}}{\EC'[n]}$. We now
  require that
  $\checksk{\Gamma}{\sigs'_{|\many{t}|}}{A_{|\many{t}|}}{\EC'[\textsf{yield};
      n]}$. This follows from the assumption $\EC~\allows~\textsf{yield}$, which
  entails that $\textsf{yield}~\in~\sigs'_{|\many{t}|}$. Thus
  $\inferskgs{\EC[\textsf{yield}; n]}{B}$.

\item[\Cse] \textsc{R-Yield-Can't}
  This case is more straightforward. By the assumption we have that the
  evaluation frame $\EF$ does not permit yielding, but the term inside the frame
  could otherwise reduce.

  Assume $\checkskgs{A}{\EF[n]}$, and therefore $\checkskgs{A'}{n}$. By the
  assumption and subject reduction, $\checkskgs{A'}{n'}$. Then clearly
  $\checkskgs{A}{\EF[n']}$.

\end{itemize}
\end{proof}




\begin{theorem}[Type Soundness]\label{thm:soundness}
\begin{itemize}
\\
\item If $\infers{\cdot}{\cdot}{\sigs}{m}{A}$ then either $m$ is a normal form
  such that $m$ respects $\sigs$ or there exists a unique
  $\infers{\cdot}{\cdot}{\sigs}{m'}{A}$ such that $m \stepstou m'$.
\item If $\checks{\cdot}{\cdot}{\sigs}{A}{n}$ then either $n$ is a normal form
  such that $n$ respects $\sigs$ or there exists a unique
  $\checks{\cdot}{\cdot}{\sigs}{A}{n'}$ such that $n \stepstoc n'$.
\end{itemize}
%% In particular, if $\sigs = \nowt$ then either the term is a value $w$ or the
%% term can reduce by one step.
\end{theorem}

\begin{proof}
The proof proceeds by simultaneous induction on
$\infers{\cdot}{\cdot}{\sigs}{m}{A}$ and $\checks{\cdot}{\cdot}{\sigs}{A}{n}$.
\end{proof}


\chapter{Implementation}
\label{chap:implementation}

We now introduce the Frank library used for asynchronous effects.
%
Our design closely follows the design of \aeff~(\cite{ahman2020asynchronous}).
\aeff~is a language designed around writing multithreaded programs that
communicate by sending \emph{interrupts}. A thread dictates how it will respond
to an interrupt by installing an \emph{interrupt handler}.
%
%% These are analogous to traditional effects and handlers; an interrupt when
%% invoked is like the invocation of a command, and an interrupt handler is like an
%% effect handler.
Interrupts and interrupt handlers can be seen as a less expressive version of
effects and effect handlers; an interrupt handler describes how to behave on
receipt of an interrupt in the same way to an effect handler, but it does not
get access to the continuation of the calling code.

Interrupts and interrupt handlers have one particularly compelling feature; when
we invoke an interrupt (in the case of synchronous effects, this is just
invoking a command), we can carry on computing the rest of the code whilst we
wait for a response. This is a stark difference to traditional effects, where
the rest of the computation is blocked whilst we wait for an answer. The
programmer can then choose to await the response from interrupt, which does
block computation if an answer is not already received.

Our system untyped in that there is no tracking of which asynchronous effects are
issued in which functions, however it is typed in that the traditional Frank
effects promises can perform are tracked.

\aeff~has an effect tracking system for asynchronous effects; our system does
not. Ours does however track the effects that can be performed by interrupt
handlers.

\section{In Frank}

\aeff's interrupt handlers are manifested in Frank through the \code{Promise}
interface.

\begin{lstlisting}
interface Promise <@\greytext{[E]}@> =
      promise R : Prom R <@\greytext{[E | Promise[E|], RefState, Yield]}@>
               -> Pid R  <@\greytext{[E | Promise[E|], RefState, Yield]}@>
    | signal : Sig -> Unit
    | await R : Pid R <@\greytext{[E | Promise[E|], RefState, Yield]}@> -> R

data Prom R <@\greytext{[E]}@> = prom {Sig -> Maybe {<@\greytext{[E|]}@>R}}

data Pid X = pid (Ref (PromiseStatus X))

data PromiseStatus X = empty | done X | resume {X -> Unit}
\end{lstlisting}

\noindent We have a lot to unpack here, so we start slowly. The \code{promise R}
command is a polymorphic command, which takes a function of type \code{Sig ->
  Maybe \{[E|] R\}}. This function is an interrupt handler; it dictates what to
do on receipt of an interrupt, which is a thing of type \code{Sig}. The return
type of the interrupt handler is \code{Maybe \{[E|] R\}}; this is because the
programmer has the chance to return \code{nothing}, which will mean the promise
goes unfulfilled and waits for another message. The programmer would want to do
this on receipt of other types of message, or if a certain condition regarding
the interrupt is not fulfilled\footnote{Interrupt handlers which put conditions
  on the incoming interrupts are called \emph{guarded} interrupt handlers --- we
  come back to these later.}. The \code{promise} operation returns a \code{Pid
  R} (promise id), which is used to check if the installed promise has completed
or not. The \code{R} type parameter is determined by the return type of the
interrupt handler. This is later used in \code{await}.


\code{signal} is a more simple operation. The \code{Sig} data type is the type
of signals that the thread can invoke. These can have extra bits of information
--- also called \emph{payloads} --- such as the parameters for a remote request.
The handler for \code{Promise} will then send the signals to each other thread,
possibly executing the interrupt handler if needs be.

\code{await} takes a \code{Pid R} and returns a value of type \code{R}. This
\code{R} is the returned value of the promise. If the promise has been fulfilled
then the promise status stored in \code{Pid} will be \code{done res}; hence we
just return this value. If it is incomplete --- i.e. the promise status is
\code{empty} --- we add the resumption to \code{resume}.

\paragraph*{Effect Typing}

We can track and control the effects that promises can perform using Frank's
effect type system. For instance, a Frank program of type \code{[Promise
    [Console]] X} can install promises that print to the console, a program of
type \code{[Promise[Console, Web]] X} can install promises that perform web
requests and print to the console, etc.

Note however that the effect typing is slightly complicated in the definition of the
interface; a type of \code{Promise [Console]} means that the installed promise
can really perform the effects \code{[Promise[Console], Console, RefState,
    Yield]}. This is expressed by the \code{[E | Promise[E|], RefState, Yield]}
part of the \code{promise R} definition. A recursive type is needed as the
promises can themselves invoke other promises.

\todo{Flesh this out, rewrite it}

\paragraph*{Threads}

%% In order to handle these operations, we need to maintain the state of the
%% executing threads somewhere; when we stop executing a thread we store its status
%% so far and start a new one. We also need ot

In order to run threads in parallel, we need to maintain a collection of
thread states; when we stop executing a thread we store its continuation so far
and start executing a new one, in the same style as Section
\ref{sec:concurrency}.

We also need to store in this collection the promises that each thread has
installed. These are stored as a stack so as to maintain the order of
installation. Installing a promise is as straightforward as pushing it
onto the corresponding thread's promise stack.


\begin{lstlisting}
data Threads <@\greytext{[E]}@> =
    tentry Int
           {<@\greytext{[E| RefState, Yield]}@> Unit}
           (TStack {Sig -> {<@\greytext{[E| RefState, Yield]}@> Unit}
                        -> <@\greytext{[E| RefState, Yield]}@> Maybe {<@\greytext{[E| RefState, Yield]}@> Unit}})
           (Threads [E|])
    | tnil
\end{lstlisting}

This collection is realised in Frank as the \code{Threads} datatype. It is
essentially just an integer map, where keys are the thread IDs and the values
are the trio of the thunked computation thus far, the stack of installed
promises at this thread, and then the rest of the map.

The type of things called in the stack is not what we would expect. Recall that
the type of Promises is \code{Sig -> Maybe {R}}, yet the promises stored here
are of type \code{Sig -> {Unit} -> Maybe {Unit}}. We address this in Section
\ref{sec:handling-promises}, where it will become clearer.

\section{Handling Promises}
\label{sec:handling-promises}

We now introduce the handler for \code{Promise} effects. The type of this comes
as no surprise;

\begin{lstlisting}
hdl : {Int -> Ref (Threads <@\greytext{[E|]}@>)
    -> <Promise<@\greytext{[E|]}@>> Unit
    -> <@\greytext{[E| RefState, Yield]}@> Unit}
\end{lstlisting}

The first argument is the id of the thread being handled. The second one is a
reference to the globally stored threads. These are parametrised by the effects
performed in the promises, just like the \code{Promise} interface. The third
argument is the thread itself, which performs \code{Promise} operations.
Finally, the return type expresses that this code can perform any of the effects
that the promises perform, plus \code{RefState} effects; the \code{Yield}
encodes that this can be interrupted (as in Chapter~\ref{chap:preemptive-concurrency}).

\begin{lstlisting}[numbers=left]
hdl thId thrs <promise (prom cb) -> k> =
    let cell = pid (new waiting) in
    let cbMod = (to_write cell cb) in
    let cbMaybe = {sig rest -> case (cbMod sig)
          { nothing -> nothing
          | (just susp) -> just { hdl thId thrs susp!; <@\greytext{<Promise>}@> rest!} }} in
    let queued = (addCb thId cbMaybe (read thrs)) in
    let res = {hdl thId thrs (k cell)} in
    write thrs (writeThk thId res queued);
    res!
\end{lstlisting}

Above we see the handler for \code{promise}. This is the process of installing a
promise, onto the currently executing thread's stack. Line 2 creates a new
reference cell for the promise id; this is initialised to \code{waiting}, as
nothing has been performed yet. Line 3 calls the utility function
\code{to_write}. This takes a callback of type \code{S -> R} and converts it to
type \code{S -> [RefState]Unit}; it modifies it to always return unit but write
its value to the \code{Pid} cell. Having all promises as the same type makes
storing them in one data structure possible. In lines 4-6 we convert our promise
to one that takes into account the computation after we've run the promise as
well. This is essential to get blocking via \code{await} to work properly when
running a promise, which is important for pre-emptive concurrency and other
applications. \todo{Go into more depth about this}.

Lines 7 to 10 are then mainly housekeeping. We write this modified promise into
the stack, update the execution state thus far and then continue handling the
thread.

\todo{Is all of the business with writing the rest of the callback needed?}

\todo{It's not - can remove it. }

\begin{lstlisting}
hdl thId thrs <signal sig -> thr> =
    let newThrs = (runThreads sig (read thrs)) in

    write thrs newThrs;
    hdl thId thrs (thr unit)
\end{lstlisting}

When a signal comes in, we need to try and execute \emph{all} of the installed
promises, for every thread. To do this we use the \code{runThreads} function,
which calls the below function for all threads;

\begin{lstlisting}
runThread sig (trio susp cbs skipped) =
    case (dequeue cbs)
        { nothing -> trio susp cbs skipped
        | (just (pair cb cbs)) ->
              case (cb sig susp)
                  { nothing -> runThread sig (trio susp cbs (enqueue cb skipped))
                  | (just susp2) -> runThread sig (trio susp2 cbs skipped)}}
\end{lstlisting}

\noindent We first check to see if there are any installed promises remaining.
If there is, we run the promise with the signal supplied. Recall that the type
of the callbacks is \code{Sig -> \{Unit\} -> \{R\}}; we have to also supply the
callbacks with the thunked computation so far. Again, this is important to
correctly implement blocking. If the callback returns \code{nothing} we do not
evaluate it and we reinstall it afterwards, by putting it onto the ``skipped''
stack.

Once promise execution is finished we update the state of \code{thrs} and resume
handling, restarting the continuation \code{thr} with \code{unit} immediately.

\begin{lstlisting}
hdl thId thrs <await cell -> thr> =
    case (readPid cell)
        { (done x) ->
            hdl thId thrs (thr x)
        | waiting ->
            writePid cell (resume thr);
            hdl thId thrs unit }
\end{lstlisting}

Handling \code{await} is surprisingly the simplest of the lot. Recall that
\code{await} takes a promise id cell \code{Pid R} and returns a value of type
\code{R}. The handler looks inside this cell; if there is a finished value there
already (\code{done x}) it resumes the continuation with this value. If the
promise has not yet completed, we then write the resumption (which is of type
\code{\{R -> Unit\}}) to the cell. The function \code{to_write} used when
installing promises changes the original promise to resume the continuation
stored in \code{Pid}, if there is one present.

\todo{Should probably show to_write.}

\section{Multithreading}

We now show how we can easily introduce multithreading to our system. Observe
that we are yet to have a handler for any \code{yield} commands. We can handle
them, yielding co-operative concurrency in the same style as
Section~\ref{sec:concurrency}, like so;

\begin{lstlisting}[numbers=left]
scheduleT : {<Yield> Unit -> Int -> Ref (Threads <@\greytext{[E|]}@>) -> <@\greytext{[E| RefState]}@>Unit}
scheduleT <yield -> k> cur thrs =
    let next = nextId cur (keys (read thrs)) in
    let newThk = lookupThk next (read thrs) in
    let newThrs = writeThk cur {k unit} (read thrs) in
    write thrs newThrs;
    scheduleT newThk! next thrs

scheduleT unit cur thrs = scheduleT yield! cur thrs
\end{lstlisting}

\noindent Recall that the threads are stored with a thread id, an integer. We use these in
our simple scheduling strategy, where we just cycle through all ids in ascending
order. Line 3 finds the id of the next thread as per this strategy, and line 4
looks up the thunk from the threads structure. Line 5 then writes the current
thread's continuation to the structure. Line 6 writes the updated version of the
threads and then line 7 starts executing the next continuation. Line 9 states
that if a thread's value is unit we just force a yield. This is useful if a
thread is e.g. blocked and waiting for a promise to fulfill, as we will
straightaway stop processing it and start the next one.

\todo{Talk about how we're still tied into the threads structure; disadvantage}

\chapter{Examples}
\label{chap:examples}

\section{Pre-emptive Concurrency}

An essential feature of our asynchronous effects system is that it supports
pre-emptive concurrency; that is, the suspension and resumption of threads
non-cooperatively. Naturally, this relies on the insertion of yields as
discussed in Chapter~\ref{chap:preemption}.

We supplement the signals supported in our program with two more;

\begin{lstlisting}
data Sig = <@\ldots@> | stop Int | go Int
\end{lstlisting}

The integer payload can act as a counter, or as a way to tell specific threads
to stop or go. The blocking or non-blocking behaviour then depends on the
promises for these signals.

\begin{lstlisting}
onStop : {Int -> <@\greytext{[Promise[Console], Console, Yield, RefState]}@> Unit}
onStop id =
    let gp = promise (prom {s -> goPromise id s}) in
    await gp;
    promise (prom {s -> stopPromise id s});
    unit

stopPromise : {Int -> Sig -> Maybe {<@\greytext{[Promise[Console], Console, Yield, RefState]}@>Unit}}
stopPromise id (stop n) =
   if (n == id)
     { just { onStop id } }
     { nothing }
stopPromise id _ = nothing
\end{lstlisting}

\code{stopPromise} is another guarded interrupt handler; it will only fire its
body if the payload to \code{stop} is the id of the thread. The body is then
fairly simple; it installs a promise waiting for \code{go} and immediately
starts blocking. The rest of the computation can not proceed until the
corresponding go message is received. Once the go promise is fulfilled, the
non-blocking stop promise is reinstalled.

\begin{lstlisting}
goPromise : {Int -> Sig -> Maybe {<@\greytext{[Promise[Console], Console, Yield, RefState]}@>Unit}}
goPromise id (go n) =
    if (n == id)
      { just {unit} }
      { nothing }
goPromise id _ = nothing
\end{lstlisting}

\code{goPromise} is simple in comparison; if it receives the correct \code{go} signal
it just returns \code{unit}.

We can then make a function pre-emptible by just installing a stop-waiting promise
in front of the function code;

\begin{lstlisting}
counter : {Int -> <@\greytext{[Console, Yield]}@>Unit}
counter x = ouint x; print " "; sleep 200000; counter (x + 1)

thread1 : {<@\greytext{[Promise[Console], Console, RefState, Yield]}@> Unit}
thread1! = promise (prom {s -> stopPromise 0 s}); counter 0
\end{lstlisting}

Observe that all we have to do is precompose with the promise installer; the
rest of the code goes on unaware that it is being pre-empted. Threads can also
then communicate etc on top of this.

\todo{Is this example even interesting now that we've already got it baked into
  the language?}

\section{Async-Await}

Here we show how our asynchronous effects system can express the familiar
async-await abstraction.

Consider that we want to asynchronously run web requests using the built-in
\code{getRequest} operation. These return a value of type \code{String}, being
hte result of the request. First we add two more signals to our set of available
signals;

\begin{lstlisting}
data Sig = <@\ldots@> | call {String} Int | result String Int
\end{lstlisting}

\todo{Observe that this is a higher-order effect - something aeff lacks!}

%% \noindent The thunked argument to \code{call} is the computation we want to run.

\noindent The signal \code{call} is used to start an asynchronous operation; the
thunked argument is the computation we want to run. \code{result} is the signal
used by the running thread to indicate that it has completed the computation and
is returning the \code{String} result. The \code{Int} arguments are for call
IDs, so that the wrong results are not re-read.

Unlike other implementations, the Frank realisation of async-await does not
dynamically create new threads to run asynchronous tasks. Instead, we have a
dedicated thread that only performs these asynced processes. This may seem
inefficient, however see that when not executing a process the thread will be
instantly skipped in the scheduler, so we have no overhead costs.

We how show the \code{async} function that a caller would use to issue a new
asynchronous task;

\begin{lstlisting}
resultWaiter : {Int -> <@\greytext{[Promise[Web, Console]]}@> Pid String <@\greytext{[WebThreads]}@>
resultWaiter callNo =
  promise (prom { (result res callNo') -> if (callNo == callNo') { just {res} } {nothing}
                  | _ -> nothing})

async : {<@\greytext{[Console, Web]}@> String} -> Ref Int ->
    <@\greytext{[WebThreads]}@> Pid String <@\greytext{[WebThreads]}@>
async proc callCounter =
    let callNo = read callCounter in
    let waiter =
      <@\greytext{<Console, RefState, Web, Yield>}@>(resultWaiter callNo) in
    signal (call proc callNo);
    write callCounter (callNo + 1);
    waiter
\end{lstlisting}

So \code{async} takes the process to be run and a reference to the callcounter.
It then installs another promise, \code{resultWaiter}, which waits for the
corresponding \code{result} signal to be received. \code{resultWaiter} is an
example of a \emph{guarded} interrupt handler; it only fires if a certain
condition regarding to the signal's payload holds (i.e. that \code{callNo ==
  callNo'}). After installing \code{resultWaiter}, \code{async} sends a
\code{call} signal with the process and callNo as argument, increments the call
counter and returns the result-waiting promise.

\begin{lstlisting}
onRun : {{<@\greytext{[Console, Web]}@> String} -> Int -> <@\greytext{[WebThreads]}@> Unit}
onRun proc callId =
    let res = <@\greytext{<Promise, RefState, Yield>}@> proc! in
    signal (result res callId);
    <@\greytext{<Console, RefState, Web, Yield>}@> runner!;
    unit

runner : {[Promise[Web, Console]] Pid Unit [WebThreads]}
runner! =
    promise (prom {(call proc callId) -> just {onRun proc callId}
                  | _ -> nothing})
\end{lstlisting}

\code{runner} is the process that runs on the worker thread. This simply
installs a promise that responds to \code{call} signals. On receipt of a
\code{call} it runs the delivered process synchronously; once it is finished it
sends a \code{result} signal and then finally reinvokes the \code{runner}.
%
Note that \code{proc} can still have \code{yield} calls inserted into it, so
that this doesn't cause the whole program to block.

\todo{Example of how it gets used.}

\section{Futures}
\label{sec:futures}

Our developed asynchronous effects system is expressive enough to implement the
asychronous post-processing of results, or \emph{futures}, on top of what we
already have. Previously these have had to be implemented as a separate language
feature. \todo{Reference for being a separate feature!}

Futures are useful if we want to asynchronously perform some action once another
promise has been completed. In the context of a web application, this might be
updating the application's display once some remote call for data has finished.
Observe that this differs from just awaiting the remote call and then updating
once we have this; we do not want to block everything else from running, but
want to perform this action asynchronously, when the promise is complete.

\begin{lstlisting}
futureList : {Pid R <@\greytext{[E|]}@> -> {R -> <@\greytext{[E|]}@> Z} -> Sig -> Maybe {<@\greytext{[E|]}@> Z}}
futureList p comp (listSig _) =
    just { let res = await p in comp res}
futureList _ _ _ = nothing
\end{lstlisting}

\noindent When calling \code{futureList} we supply a promise of result type
\code{R} and a computation of type \code{R -> Z}. We then await the promise, and
once we have a value (of type \code{R}) run the computation with this. An
example computation using this system is;

\begin{lstlisting}
let recv = promise { (listSig xs) -> just {xs} | _ -> nothing} in
let prod = promise {s -> futureList filt product s} in
promise {s -> futureList prod {x -> signal (resultSig x)} s}
\end{lstlisting}

\noindent Where we, upon receipt of a list signal, take the product of the list
element-wise and send another signal with this result. All three of these
promises are triggered by the same signal; \code{recv} is executed first, which
then executes \code{prod}, which then lets the final one run. This behaviour
depends on signals being able to execute many promises at once (that is,
behaving like \emph{deep} rather than shallow handlers).

\section{Cancelling Tasks}

\todo{Link back to the async/await w/ dynamic}

Because we are working in a language equipped with effect handlers, we can
easily write a handler for the \textsf{Cancel} effect, which just gets rid of
the continuation and replaces it with some default value (e.g. \code{unit}).

\begin{lstlisting}
  interface Cancel = cancel : Unit
 
  hdlCancel : {<Cancel> Unit -> Unit}
  hdlCancel <cancel -> _> = unit
  hdlCancel unit          = unit
\end{lstlisting}

We can use this to cancel a task issued with \code{async}. Recall that these
tasks run on their own thread. As such, we can just cancel the entire thread at
the top-level.

We have to modify the handler for the \textsf{Promise} effect for this. Recall
that when we install a promise, we convert it to a form that takes the rest of
the computation and reinstalls the promise handler. We need to then wrap this in
a handler for \textsf{Cancel} effects again; this is because user-level promises
could perform \textsf{Cancel} effects. Thus, we convert them to

\begin{lstlisting}
case (cbMod sig)
    { nothing -> nothing
    | (just susp) ->
      just { stopCancel (hdl thId thrs (susp!; <LCancel, Promise> rest!)) }}
\end{lstlisting}

\todo{Talk about informing the scheduler that the thread has been cancelled as well.}

The realisation of cancellable function calls
in~\aeff(\cite{ahman2020asynchronous}) was to start awaiting a new promise that
will never be installed. This leads to a space leak as unfulfilled promises
build up. Our approach improves on this as the cancelled calls do genuinely
disappear.

However, a weakness of ours is that we have to modify the handler code for
promises, even though cancellation of calls and promises should be orthogonal.

\section{Interleaving}

With the \textsf{Cancel} effect, we can also define the useful \code{interleave}
combinator, in the spirit of Koka's interleave
operator~\cite{leijen2017structured}.

\todo{Think of a better way to make reference to Daan's work}

\begin{lstlisting}
interleave : {{[InTask] String} -> {[InTask] String}
          -> Ref Int -> [WebEffs] Pid String [WebEffs]}
interleave procA procB callCounter =
    -- Issue the two calls with signals.
    let callNoA = read callCounter in
    write callCounter (callNoA + 1);
    let callNoB = read callCounter in
    write callCounter (callNoB + 1);

    -- Install a waiter to wait for the first result
    let ileaveWaiter = (leaveResWaiter callNoA callNoB) in

    -- Signal to start the other two
    signal (call procA callNoA);
    signal (call procB callNoB);

    -- Just return waiter.
    ileaveWaiter
\end{lstlisting}

This will set two threads running on independent threads. It then installs an
interrupt handler for result messages; whichever result returns first, we cancel
the other task and just return the result of the original one.

This lets us write timeouts for functions; we can cancel a task if it takes too
long to return (e.g. a request to a web server that is inactive), or run two
remote identical requests to different services and just take the result of
whichever one returns first. This also generalises to the $n$-ary case in the
natural way.



\chapter{Conclusion}
\label{chap:conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BIBLIOGRAPHY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliographystyle{plainnat}
\bibliography{bibliography}

%% You can include appendices like this:
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\appendix

\chapter{Remaining Formalisms}

\begin{figure} % \figrule
\flushleft
%% $\boxed{\kindchecksk{R}}$
%% \begin{mathpar}
%% \inferrule[K-Val]
%%   {\TyVar(A) \subseteq \kenv}
%%   {\kindchecksk{A}}

%% \inferrule[K-Eff]
%%   {\TyVar(\sigs) \subseteq \kenv}
%%   {\kindchecksk{[\sigs]}}
%% %
%% \end{mathpar}

$\boxed{\infersk{\Gamma}{\sigs}{m}{A}}$
\begin{mathpar}
\inferrule[T-Var]
  {
   x:A \in \Gamma}
  {\inferskgs{x}{A}}

\inferrule[T-PolyVar]
  {% \kindchecks{\kenv, \many{Z}}{A}\\
   \kindchecksk{\many{R}} \\
   f:\forall \many{Z}.A \in \Gamma}
  {\inferskgs{f~\many{R}}{A[\many{R}/\many{Z}]}}
\\
\inferrule[T-App]
  {\sigs' = \sigs \\
   (\adjact{\sigs}{\adj_i}{\sigs'_i})_i \\\\
   \inferskgs{m}{\thunk{\many{\effin{\adj}A \to}~ \effout{\sigs'}B}} \\
   (\checksk{\Gamma}{\sigs'_i}{A_i}{n_i})_i}
  {\infersk{\Gamma}{\sigs}{m~\many{n}}{B}}

\inferrule[T-Ascribe]
  {\checkskgs{A}{n}}
  {\inferskgs{\cu (n : A)}{A}}
%
\end{mathpar}

$\boxed{\checksk{\Gamma}{\sigs}{A}{n}}$
\begin{mathpar}
\inferrule[T-Switch]
  {\inferskgs{m}{A} \\ A = B}
  {\checkskgs{B}{\uc m}}

\inferrule[T-Data]
  {%(\kindchecksk{R_i})_i\\
   k~\many{A} \in D~\many{R} \\
   (\checkskgs{A_j}{n_j})_j}
  {\checkskgs{D~\many{R}}{k~\many{n}}}

\inferrule[T-Command]
  {\kindchecksk{\many{R}} \\
   c : \forall \many{Z}.\many{A \to}~ B \in \sigs \\
   (\checkskgs{A_j[\many{R}/\many{Z}]}{n_j})_j}
  {\checkskgs{B[\many{R}/\many{Z}]}{c~\many{R}~\many{n}}}

\inferrule[T-Thunk]
  {\checksdefkg{C}{e}}
  {\checkskgs{\thunk{C}}{\thunk{e}}}

\inferrule[T-Let]
  {P = \forall \many{Z}.A \\\\
   \checkbase{\kenv, \many{Z}}{\sigentails{\emptyset}}{\Gamma}{A}{n} \\
   \checksk{\Gamma, f : P}{\sigs}{B}{n'}}
  {\checkskgs{B}{\key{let}~f : P = n~\key{in}~n'}}

\inferrule[T-LetRec]
  {(P_i = \forall \many{Z}_i.\thunk{C_i})_i \\\\
   (\checkbase{\kenv, \many{Z}_i}{\vdash}{\Gamma, \many{f : P}}{C}{e_i})_i\\
   \checksk{\Gamma, \many{f : P}}{\sigs}{B}{n}}
  {\checkskgs{B}{\key{letrec}~\many{f : P = e}~\key{in}~n}}

\inferrule[T-Adapt]
  {\adjact{\sigs}{\adapt}{\sigs'} \\ \checksk{\Gamma}{\sigs'}{A}{n}}
  {\checkskgs{A}{\effin{\adapt}~n}}
\end{mathpar}

$\boxed{\checksdefkg{C}{e}}$
\begin{mathpar}
\inferrule[T-Comp]
  {(\matchesck{T_j}{r_{i,j}}{\sigs}{\exists \kenva_{i,j}.\Gamma'_{i,j}})_{i,j} \\
   (\checks{\kenv, (\kenva_{i,j})_j}{\Gamma, (\Gamma'_{i,j})_j}{\sigs}{B}{n_i})_i \\
   ((r_{i,j})_{i} \text{ covers } T_j)_j}
  {\checksdefkg{(T_j \to)_j~\effout{\sigs}B}{((r_{i,j})_j \mapsto n_i)_i}}
\end{mathpar}
\caption{Term Typing Rules}
\label{fig:term-typing}
% \figrule
\end{figure}

\begin{figure}%% \figrule
\flushleft
$\boxed{\adjact{\sigs}{\adj}{\sigs'}}$
\begin{mathpar}
\inferrule[A-Adj]{\adjact{\sigs}{\adapt}{\sigs'} \\
  \adjact{\sigs'}{\ext}{\sigs''}}
          {\adjact{\sigs}{\adapt\pipe\ext}{\sigs''}}
\end{mathpar}
$\boxed{\adjact{\sigs}{\ext}{\sigs'}}$
\begin{mathpar}
\inferrule[A-Ext-Id]{ }
          {\adjact{\sigs}{\id}{\sigs}}

\inferrule[A-Ext-Snoc]{\adjact{\sigs}{\ext}{\sigs'} }
          {\adjact{\sigs}{\ext, \sig~\many{R}}{\sigs', \sig~\many{R}}}
\end{mathpar}
$\boxed{\adjact{\sigs}{\adapt}{\sigs'}}$
\begin{mathpar}
\inferrule[A-Adapt-Id]{ }
          {\adjact{\sigs}{\id}{\sigs}}

\inferrule[A-Adapt-Snoc]{\adjact{\sigs}{\adapt}{\sigs'} \\
    \adpcom{\sigs'}{\sig}{S}{S'}{\sigs''}}
          {\adjact{\sigs}{\adapt, \sig(S \to S')}{\sigs''}}
\end{mathpar}
$\boxed{\adpcom{\sigs}{\sig}{S}{S'}{\sigs'}}$
\begin{mathpar}
\inferrule[A-Adapt-Com]
  {\itrbnd{\sigs}{S}{\sig}{\sigs'}{\ienv} \\
   \itrinst{\ienv}{S'}{\sig}{\ext} \\
   \adjact{\sigs'}{\ext}{\sigs''}}
  {\adpcom{\sigs}{\sig}{S}{S'}{\sigs''}}
\end{mathpar}

$\boxed{\itrbnd{\sigs}{S}{\sig}{\sigs'}{\ienv}}$
\begin{mathpar}
\inferrule[I-Pat-Id]{ }
          {\itrbnd{\sigs}{\pid}{\sig}{\sigs}{s : \sigs}}

\inferrule[I-Pat-Bind]{\itrbnd{\sigs}{S}{\sig}{\sigs'}{\ienv}}
          {\itrbnd{\sigs,\sig~\many{R}}{S~a}{\sig}{\sigs'}
            {\ienv,a:\sig~\many{R}}}

\inferrule[I-Pat-Skip]{
  \itrbnd{\sigs}{S~a}{\sig}{\sigs'}{\ienv} \\
  \sig \neq \sig'}
  {\itrbnd{\sigs,\sig'~\many{R}}{S~a}{\sig}
          {\sigs',\sig'~\many{R}}{\ienv}}
\end{mathpar}


$\boxed{\itrinst{\ienv}{S}{\sig}{\ext}}$
\begin{mathpar}
\inferrule[I-Inst-Id]{s\in\meta{dom}(\ienv)}
          {\itrinst{\ienv}{\pid}{\sig}{\id}}

\inferrule[I-Inst-Lkp]{a\in\meta{dom}(\ienv) \\
  \itrinst{\ienv}{S}{\sig}{\ext} \\
  \ienv(a)=\sig~\many{R}}
          {\itrinst{\ienv}{S~a}{\sig}{\ext,\sig~\many{R}}}
\end{mathpar}
%% \caption{Action of an Adaptor's Interface Component on an Ability}
\label{fig:interface-components}

\begin{figure}[t]
%% \figrule
\flushleft
$\boxed{\inferskgs{m}{A}}$ \quad $\boxed{\checkskgs{A}{n}}$
\begin{mathpar}
\inferrule[T-Freeze-Use]
  {\neg(\EC \handles c) \\
   \inferskgs{\EC[c~\many{R}~\many{w}]}{A}}
  {\inferskgs{\freeze{\EC[c~\many{R}~\many{w}]}}{A}}

\inferrule[T-Freeze-Cons]
  {\neg(\EC \handles c) \\
   \checkskgs{A}{\EC[c~\many{R}~\many{w}]}}
  {\checkskgs{A}{\freeze{\EC[c~\many{R}~\many{w}]}}}
\end{mathpar}
\caption{Frozen Commands}
\label{fig:frozen-typing}
%% \figrule
\end{figure}

\caption{Action of an Adjustment on an Ability and Auxiliary Judgements}
\label{fig:act-adj}
%% \figrule
\end{figure}


\begin{figure} % \figrule
\flushleft

\[
\mathcal{X} ::= A \gor C \gor T \gor G \gor Z \gor R \gor P
                  \gor \seed \gor \sigs \gor \ext \gor \adapt \gor \adj
                  \gor \Gamma \gor \exists \kenva.\Gamma \gor \ienv
\]

$\boxed{\kindchecksk{\mathcal{X}}}$
%% \boxed{\kindchecksk{C}}\boxed{\kindchecksk{T}}
%% \boxed{\kindchecksk{G}}\boxed{\kindchecksk{Z}}\boxed{\kindchecksk{R}}\boxed{\kindchecksk{P}}
%% \boxed{\kindchecksk{\seed}}\boxed{\kindchecksk{\sigs}}
%% \boxed{\kindchecksk{\ext}}\boxed{\kindchecksk{\adapt}}\boxed{\kindchecksk{\adj}}
%% \boxed{\kindchecksk{S}}\boxed{\kindchecksk{\Gamma}}\boxed{\kindchecksk{\ienv}}
%% $
\begin{mathpar}
\inferrule[WF-Val]
  { }
  {\kindchecks{\kenv, X}{X}}

\inferrule[WF-Eff]
  { }
  {\kindchecks{\kenv, [E]}{E}}

\inferrule[WF-Poly]
  {\kindchecks{\kenv, \many{Z}}{A}}
  {\kindchecks{\kenv}{\forall \many{Z}.A}}
\\
\inferrule[WF-Data]
  {(\kindchecksk{R})_i}
  {\kindchecksk{D~\many{R}}}

\inferrule[WF-Thunk]
  {\kindchecksk{C}}
  {\kindchecksk{\thunk{C}}}

\inferrule[WF-Comp]
  {(\kindchecksk{T})_i \\ \kindchecksk{G}}
  {\kindchecksk{\many{T \to}~ G}}

\inferrule[WF-Arg]
  {\kindchecksk{\adj} \\ \kindchecksk{A}}
  {\kindchecksk{\effin{\adj}A}}

\inferrule[WF-Ret]
  {\kindchecksk{\sigs} \\ \kindchecksk{A}}
  {\kindchecksk{\effout{\sigs}A}}

\inferrule[WF-Ability]
  {\kindchecksk{\sigs}}
  {\kindchecksk{[\sigs]}}

\inferrule[WF-Pure]
  { }
  {\kindchecksk{\nowt}}

\inferrule[WF-Id]
  { }
  {\kindchecksk{\id}}

\inferrule[WF-Ext]
  {\kindchecksk{\ext} \\ (\kindchecksk{R})_i}
  {\kindchecksk{\ext, \sig~\many{R}}}

\inferrule[WF-Adapt]
  {\kindchecksk{\adapt}}
  {\kindchecksk{\adapt, \sig~(S \to S')}}
\\
\inferrule[WF-Empty]
  { }
  {\kindchecksk{\cdot}}

\inferrule[WF-Mono]
  {\kindchecksk{\Gamma} \\ \kindchecksk{A}}
  {\kindchecksk{\Gamma, x : A}}

\inferrule[WF-Poly]
  {\kindchecksk{\Gamma} \\ \kindchecksk{P}}
  {\kindchecksk{\Gamma, f : P}}
\\

\inferrule[WF-Existential]
  {\kindchecks{\kenv, \kenva}{\Gamma}}
  {\kindchecksk{\exists \kenva.\Gamma}}

\inferrule[WF-Interface]
  {\kindchecksk{\ienv} \\ (\kindchecksk{R})_i}
  {\kindchecksk{\ienv, x : \sig~\many{R}}}

\end{mathpar}


\caption{Well-Formedness Rules}
\label{fig:well-formedness}

% \figrule
\end{figure}

\begin{figure} % \figrule
\flushleft
$\boxed{\matchesvk{A}{p}{\Gamma}}$
\begin{mathpar}
\inferrule[P-Var]
  { }
  {\matchesvk{A}{x}{x:A}}

\inferrule[P-Data]
  {k~\many{A} \in D~\many{R} \\
   (\matchesvk{A_i}{p_i}{\Gamma})_i}
  {\matchesvk{D~\many{R}}{k~\many{p}}{\many{\Gamma}}}
\end{mathpar}
$\boxed{\matchesck{T}{r}{\sigs}{\exists \kenva.\Gamma}}$
\begin{mathpar}
\inferrule[P-Value]
  {\adjact{\sigs}{\adj}{\sigs'} \\ \matchesvk{A}{p}{\Gamma}}
  {\matchesck{\effin{\adj}A}{p}{\sigs}{\Gamma}}

\inferrule[P-CatchAll]
  {\adjact{\sigs}{\adj}{\sigs'}}
  {\matchesck{\effin{\adj}A}{\effin{x}}{\sigs}{x:{\thunk{\effout{\sigs'}A}}}}

\inferrule[P-Command]
  {
   \adjact{\sigs}{\adj}{\sigs'} \\
   \adj = \adapt\pipe\ext \\
   c:\forall \many{Z}.\many{A \to} B \in \ext \\
   (\matchesv{\kenv, \many{Z}}{A_i}{p_i}{\Gamma_i})_i}
  {\matchesc{\kenv}
            {\effin{\adj}B'}
            {\effin{\handle{c~\many{p}}{z}}}
            {\sigs}
            {\exists \many{Z}.\many{\Gamma}, z:\{\effin{\id\pipe\id}B \to \effout{\sigs'}B'\}}}
\end{mathpar}
\caption{Pattern Matching Typing Rules}
\label{fig:pattern-typing}
%% \figrule
\end{figure}

% \section{First section}
% 
% Markers do not have to consider appendices. Make sure that your contributions
% are made clear in the main body of the dissertation (within the page limit).

\end{document}
